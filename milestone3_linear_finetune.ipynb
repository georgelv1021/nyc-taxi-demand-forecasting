{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "743a885b-99b0-419e-8fce-2662c48b1662",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Project Question: Predict the hourly demand of taxi services in each zone in Manhattan NYC.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ed6c4dc-26df-4067-bd43-27d13e5e43dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CHANGE TO YOUR EMAIL IN WORKSPACE\n",
    "user = \"zhaozhou.lyu@vanderbilt.edu\"\n",
    "# Now, should be able to run notebook fully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18b69790-87ab-4603-bf94-06f16d28404f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db943462-0079-42e1-812c-d24ee2f6ebb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, expr, dayofweek, weekofyear, date_format, asc, udf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import IntegerType\n",
    "import datetime\n",
    "from pyspark.sql.functions import col, to_timestamp, month, year\n",
    "from pyspark.sql.types import IntegerType\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import to_date, lit\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f946ec6-8526-4fc8-8e91-8496bb705468",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "190553c7-04c6-48af-a76e-39a471369379",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remote blob path: wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow\nRegister the DataFrame as a SQL temporary view: source\nDisplaying top 10 rows: \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>vendorID</th><th>tpepPickupDateTime</th><th>tpepDropoffDateTime</th><th>passengerCount</th><th>tripDistance</th><th>puLocationId</th><th>doLocationId</th><th>startLon</th><th>startLat</th><th>endLon</th><th>endLat</th><th>rateCodeId</th><th>storeAndFwdFlag</th><th>paymentType</th><th>fareAmount</th><th>extra</th><th>mtaTax</th><th>improvementSurcharge</th><th>tipAmount</th><th>tollsAmount</th><th>totalAmount</th><th>puYear</th><th>puMonth</th></tr></thead><tbody><tr><td>CMT</td><td>2012-02-29T23:53:14Z</td><td>2012-03-01T00:00:43Z</td><td>1</td><td>2.1</td><td>null</td><td>null</td><td>-73.980494</td><td>40.730601</td><td>-73.983532</td><td>40.752311</td><td>1</td><td>N</td><td>CSH</td><td>7.3</td><td>0.5</td><td>0.5</td><td>null</td><td>0.0</td><td>0.0</td><td>8.3</td><td>2012</td><td>3</td></tr><tr><td>VTS</td><td>2012-03-17T08:01:00Z</td><td>2012-03-17T08:15:00Z</td><td>1</td><td>11.06</td><td>null</td><td>null</td><td>-73.986067</td><td>40.699862</td><td>-73.814838</td><td>40.737052</td><td>1</td><td>null</td><td>CRD</td><td>24.5</td><td>0.0</td><td>0.5</td><td>null</td><td>4.9</td><td>0.0</td><td>29.9</td><td>2012</td><td>3</td></tr><tr><td>CMT</td><td>2012-02-29T23:58:51Z</td><td>2012-03-01T00:15:48Z</td><td>1</td><td>3.4</td><td>null</td><td>null</td><td>-73.968967</td><td>40.754359</td><td>-73.957048</td><td>40.743289</td><td>1</td><td>N</td><td>CRD</td><td>12.5</td><td>0.5</td><td>0.5</td><td>null</td><td>1.5</td><td>0.0</td><td>15.0</td><td>2012</td><td>3</td></tr><tr><td>CMT</td><td>2012-03-01T19:24:16Z</td><td>2012-03-01T19:31:22Z</td><td>1</td><td>1.3</td><td>null</td><td>null</td><td>-73.99374</td><td>40.75307</td><td>-74.005428</td><td>40.741118</td><td>1</td><td>N</td><td>CRD</td><td>6.1</td><td>1.0</td><td>0.5</td><td>null</td><td>0.0</td><td>0.0</td><td>7.6</td><td>2012</td><td>3</td></tr><tr><td>CMT</td><td>2012-02-29T23:46:32Z</td><td>2012-03-01T00:05:18Z</td><td>3</td><td>2.0</td><td>null</td><td>null</td><td>-73.973723</td><td>40.752323</td><td>-73.948275</td><td>40.769413</td><td>1</td><td>N</td><td>CSH</td><td>11.7</td><td>0.5</td><td>0.5</td><td>null</td><td>0.0</td><td>0.0</td><td>12.7</td><td>2012</td><td>3</td></tr><tr><td>VTS</td><td>2012-03-07T15:17:00Z</td><td>2012-03-07T15:26:00Z</td><td>5</td><td>1.87</td><td>null</td><td>null</td><td>-73.988237</td><td>40.75929</td><td>-73.97114</td><td>40.78275</td><td>1</td><td>null</td><td>CSH</td><td>7.7</td><td>0.0</td><td>0.5</td><td>null</td><td>0.0</td><td>0.0</td><td>8.2</td><td>2012</td><td>3</td></tr><tr><td>CMT</td><td>2012-02-29T23:41:58Z</td><td>2012-03-01T00:02:29Z</td><td>1</td><td>12.4</td><td>null</td><td>null</td><td>-73.954536</td><td>40.727742</td><td>-73.768994</td><td>40.760246</td><td>1</td><td>N</td><td>CSH</td><td>28.5</td><td>0.5</td><td>0.5</td><td>null</td><td>0.0</td><td>0.0</td><td>29.5</td><td>2012</td><td>3</td></tr><tr><td>VTS</td><td>2012-03-18T15:21:00Z</td><td>2012-03-18T15:32:00Z</td><td>6</td><td>2.51</td><td>null</td><td>null</td><td>-74.001705</td><td>40.732345</td><td>-73.974888</td><td>40.750835</td><td>1</td><td>null</td><td>CSH</td><td>8.9</td><td>0.0</td><td>0.5</td><td>null</td><td>0.0</td><td>0.0</td><td>9.4</td><td>2012</td><td>3</td></tr><tr><td>CMT</td><td>2012-02-29T23:47:08Z</td><td>2012-03-01T00:06:42Z</td><td>4</td><td>6.3</td><td>null</td><td>null</td><td>-73.992319</td><td>40.724503</td><td>-73.923589</td><td>40.76113</td><td>1</td><td>N</td><td>CRD</td><td>16.5</td><td>0.5</td><td>0.5</td><td>null</td><td>4.37</td><td>0.0</td><td>21.87</td><td>2012</td><td>3</td></tr><tr><td>VTS</td><td>2012-03-13T22:26:00Z</td><td>2012-03-13T22:37:00Z</td><td>1</td><td>1.34</td><td>null</td><td>null</td><td>-74.009907</td><td>40.706292</td><td>-74.000512</td><td>40.71733</td><td>1</td><td>null</td><td>CSH</td><td>7.3</td><td>0.5</td><td>0.5</td><td>null</td><td>0.0</td><td>0.0</td><td>8.3</td><td>2012</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "CMT",
         "2012-02-29T23:53:14Z",
         "2012-03-01T00:00:43Z",
         1,
         2.1,
         null,
         null,
         -73.980494,
         40.730601,
         -73.983532,
         40.752311,
         1,
         "N",
         "CSH",
         7.3,
         0.5,
         0.5,
         null,
         0.0,
         0.0,
         8.3,
         2012,
         3
        ],
        [
         "VTS",
         "2012-03-17T08:01:00Z",
         "2012-03-17T08:15:00Z",
         1,
         11.06,
         null,
         null,
         -73.986067,
         40.699862,
         -73.814838,
         40.737052,
         1,
         null,
         "CRD",
         24.5,
         0.0,
         0.5,
         null,
         4.9,
         0.0,
         29.9,
         2012,
         3
        ],
        [
         "CMT",
         "2012-02-29T23:58:51Z",
         "2012-03-01T00:15:48Z",
         1,
         3.4,
         null,
         null,
         -73.968967,
         40.754359,
         -73.957048,
         40.743289,
         1,
         "N",
         "CRD",
         12.5,
         0.5,
         0.5,
         null,
         1.5,
         0.0,
         15.0,
         2012,
         3
        ],
        [
         "CMT",
         "2012-03-01T19:24:16Z",
         "2012-03-01T19:31:22Z",
         1,
         1.3,
         null,
         null,
         -73.99374,
         40.75307,
         -74.005428,
         40.741118,
         1,
         "N",
         "CRD",
         6.1,
         1.0,
         0.5,
         null,
         0.0,
         0.0,
         7.6,
         2012,
         3
        ],
        [
         "CMT",
         "2012-02-29T23:46:32Z",
         "2012-03-01T00:05:18Z",
         3,
         2.0,
         null,
         null,
         -73.973723,
         40.752323,
         -73.948275,
         40.769413,
         1,
         "N",
         "CSH",
         11.7,
         0.5,
         0.5,
         null,
         0.0,
         0.0,
         12.7,
         2012,
         3
        ],
        [
         "VTS",
         "2012-03-07T15:17:00Z",
         "2012-03-07T15:26:00Z",
         5,
         1.87,
         null,
         null,
         -73.988237,
         40.75929,
         -73.97114,
         40.78275,
         1,
         null,
         "CSH",
         7.7,
         0.0,
         0.5,
         null,
         0.0,
         0.0,
         8.2,
         2012,
         3
        ],
        [
         "CMT",
         "2012-02-29T23:41:58Z",
         "2012-03-01T00:02:29Z",
         1,
         12.4,
         null,
         null,
         -73.954536,
         40.727742,
         -73.768994,
         40.760246,
         1,
         "N",
         "CSH",
         28.5,
         0.5,
         0.5,
         null,
         0.0,
         0.0,
         29.5,
         2012,
         3
        ],
        [
         "VTS",
         "2012-03-18T15:21:00Z",
         "2012-03-18T15:32:00Z",
         6,
         2.51,
         null,
         null,
         -74.001705,
         40.732345,
         -73.974888,
         40.750835,
         1,
         null,
         "CSH",
         8.9,
         0.0,
         0.5,
         null,
         0.0,
         0.0,
         9.4,
         2012,
         3
        ],
        [
         "CMT",
         "2012-02-29T23:47:08Z",
         "2012-03-01T00:06:42Z",
         4,
         6.3,
         null,
         null,
         -73.992319,
         40.724503,
         -73.923589,
         40.76113,
         1,
         "N",
         "CRD",
         16.5,
         0.5,
         0.5,
         null,
         4.37,
         0.0,
         21.87,
         2012,
         3
        ],
        [
         "VTS",
         "2012-03-13T22:26:00Z",
         "2012-03-13T22:37:00Z",
         1,
         1.34,
         null,
         null,
         -74.009907,
         40.706292,
         -74.000512,
         40.71733,
         1,
         null,
         "CSH",
         7.3,
         0.5,
         0.5,
         null,
         0.0,
         0.0,
         8.3,
         2012,
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "vendorID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tpepPickupDateTime",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "tpepDropoffDateTime",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "passengerCount",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "tripDistance",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "puLocationId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doLocationId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "startLon",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "startLat",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "endLon",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "endLat",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "rateCodeId",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "storeAndFwdFlag",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "paymentType",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "fareAmount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "extra",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "mtaTax",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "improvementSurcharge",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tipAmount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "tollsAmount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "totalAmount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "puYear",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "puMonth",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data in\n",
    "# Data comes from https://learn.microsoft.com/en-us/azure/open-datasets/dataset-taxi-yellow?tabs=pyspark#azure-databricks\n",
    "\n",
    "# Create spark session\n",
    "spark = SparkSession.builder.appName(\"FinalProject\").getOrCreate()\n",
    "\n",
    "# Azure storage access info\n",
    "blob_account_name = \"azureopendatastorage\"\n",
    "blob_container_name = \"nyctlc\"\n",
    "blob_relative_path = \"yellow\"\n",
    "blob_sas_token = \"r\"\n",
    "\n",
    "# Allow SPARK to read from Blob remotely\n",
    "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
    "spark.conf.set(\n",
    "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
    "  blob_sas_token)\n",
    "print('Remote blob path: ' + wasbs_path)\n",
    "\n",
    "# SPARK read parquet, note that it won't load any data yet by now\n",
    "df = spark.read.parquet(wasbs_path)\n",
    "print('Register the DataFrame as a SQL temporary view: source')\n",
    "df.createOrReplaceTempView('source')\n",
    "\n",
    "# Display top 10 rows\n",
    "print('Displaying top 10 rows: ')\n",
    "display(spark.sql('SELECT * FROM source LIMIT 10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ebda45e-367b-467d-9060-31809c778fd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load in taxi-zone-lookup.csv file because we want to focus on Manhattan data since 90% of the pickups come from there. This means we also minimize the pickup location zones from 265 to 69 pick up zones.  \n",
    "# First, loaded in the taxi-zone-lookup.csv to the workspace where this notebook is\n",
    "# file_path = f\"/Workspace/Users/{user}/taxi-zone-lookup.csv\" \n",
    "# taxi_zone = pd.read_csv(file_path)\n",
    "# taxi_zone\n",
    "\n",
    "# Filter the pick up zone df to get only Manhattan LocationIDs\n",
    "# manhattan_df = taxi_zone[taxi_zone['Borough'] == 'Manhattan']\n",
    "# Get a list of location IDs in Manhattan\n",
    "# manhattan_location_ids = manhattan_df['LocationID'].tolist()\n",
    "# manhattan_location_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbc7e99b-d349-431d-8393-9831083d0077",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Truncate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9940cf7b-4ebb-4f00-9165-38c2801a9a4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of puLocationIds to filter for Manhattan rides\n",
    "puLocationIds_manhattan = [4, 12, 13, 24, 41, 42, 43, 45, 48, 50, 68, 74, 75, 79, 87, 88, 90, 100, 103, 104, 105, 107, 113, 114, 116, 120, 125, 127, 128, 137, 140, 141, 142, 143, 144, 148, 151, 152, 153, 158, 161, 162, 163, 164, 166, 170, 186, 194, 202, 209, 211, 224, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 243, 244, 246, 249, 261, 262, 263]\n",
    "# len(puLocationIds_manhattan) # double check 69 zones. Yes\n",
    "\n",
    "# Filter for only 2017-2018 data in Manhattan \n",
    "filter_conditions = ((col(\"puYear\") >= 2017) & (col(\"puYear\") <= 2018)) & (col(\"puLocationId\").isin(puLocationIds_manhattan))\n",
    "\n",
    "df_2017_2018_manhattan = df.filter(filter_conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b516130-1ad8-47ed-8358-fbe1dfa02e05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0633c1eb-ee30-4718-a70d-d2397c003ec2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop startLon, startLat, endLon, and endLat columns. They are null & not needed for this project scope\n",
    "columns_to_drop = ['startLon','startLat', 'endLon', 'endLat']\n",
    "df_2017_2018_manhattan = df_2017_2018_manhattan.drop(*columns_to_drop)\n",
    "\n",
    "# Transform non-numeric columns to numeric data types\n",
    "df_2017_2018_manhattan = df_2017_2018_manhattan \\\n",
    "    .withColumn(\"vendorID\", col(\"vendorID\").cast(\"int\")) \\\n",
    "    .withColumn(\"puLocationId\", col(\"puLocationId\").cast(\"int\")) \\\n",
    "    .withColumn(\"doLocationId\", col(\"doLocationId\").cast(\"int\")) \\\n",
    "    .withColumn(\"paymentType\", col(\"paymentType\").cast(\"int\")) \\\n",
    "    .withColumn(\"storeAndFwdFlag\", when(col(\"storeAndFwdFlag\") == 'Y', 1).otherwise(0).cast(\"int\")) \\\n",
    "    .withColumn(\"improvementSurcharge\", col(\"improvementSurcharge\").cast(\"double\"))\n",
    "\n",
    "# Remove tripDistance less than 0 and greater than 12. \n",
    "# Remove under 1 min rides and greater than 45 min rides.\n",
    "# Remove rides with no passenegers and more than 5 passengers.\n",
    "# Remove speeds less than 4 mph and greater than 30 mph.\n",
    "# Remove fareAmount less than $4 and greater than $52.\n",
    "\n",
    "# Calculate ride time in minutes (ride_duration)\n",
    "df_2017_2018_manhattan = df_2017_2018_manhattan.withColumn('ride_duration', # in minutes\n",
    "                                       (expr('unix_timestamp(tpepDropoffDateTime) - unix_timestamp(tpepPickupDateTime)') / 60).cast('double'))\n",
    "\n",
    "# Calculate speed, remember this is NOT per hour, it is per minute so need to multiply by 60\n",
    "df_2017_2018_manhattan = df_2017_2018_manhattan.withColumn('speed', (col('tripDistance') / col('ride_duration')) * 60) # convert minutes to hours (multiply by 60)\n",
    "\n",
    "# Remove rows\n",
    "clean_df = df_2017_2018_manhattan[(df_2017_2018_manhattan['tripDistance'] >= 0) & (df_2017_2018_manhattan['tripDistance'] <= 12) & (df_2017_2018_manhattan['ride_duration'] >= 1) & (df_2017_2018_manhattan['ride_duration'] <= 45) & (df_2017_2018_manhattan['passengerCount'] >= 1) & (df_2017_2018_manhattan['passengerCount'] <= 5) & (df_2017_2018_manhattan['speed'] >= 4) & (df_2017_2018_manhattan['speed'] <= 30) & (df_2017_2018_manhattan['fareAmount'] >= 4) & (df_2017_2018_manhattan['fareAmount'] <= 52)]\n",
    "\n",
    "# Drop columns not needed\n",
    "# List of columns to drop not useful for this project scope\n",
    "columns_to_drop = [\"vendorID\",\"tpepDropoffDateTime\",\"passengerCount\",\"tripDistance\", \"doLocationId\", \"rateCodeId\", \"storeAndFwdFlag\", \"paymentType\", \"fareAmount\", \"extra\", \"mtaTax\", \"improvementSurcharge\", \"tipAmount\", \"tollsAmount\", \"totalAmount\", \"ride_duration\",\"speed\"]\n",
    "\n",
    "clean_df = clean_df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaadbc80-faa8-467d-86e3-5349632ceb53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Derive Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb504de-9029-46e4-9074-febcc1fe35e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create indication if weekday/weekend as this could affect the number of taxis needed.\n",
    "# Create indication if holiday (1) or not (0).\n",
    "# Create hour of day, day of week, and week number.\n",
    "# Create demand column that counts number of taxis per day and hour for EACH pickup zone.\n",
    "\n",
    "# Indication if weekday/weekend\n",
    "clean_df = clean_df.withColumn('is_weekend', \n",
    "                                         when((dayofweek(clean_df['tpepPickupDateTime']) == 1) | \n",
    "                                              (dayofweek(clean_df['tpepPickupDateTime']) == 7), 1).otherwise(0))\n",
    "\n",
    "# Indication if holiday\n",
    "# A list of holidays in `holiday_dates`: New Years, MLK day, Presidents day, Memorial Day, 4th of July, Labor Day, Thanksgiving, and Christmas\n",
    "holiday_dates = ['2017-01-01', '2018-01-01', '2017-01-16', '2018-01-15','2017-02-20','2018-02-19','2017-05-29', '2018-05-28','2017-07-04', '2018-07-04','2017-09-4','2018-09-03','2017-11-23', '2018-11-22', '2017-12-25', '2018-12-25'\n",
    "]  \n",
    "# 1 if holiday, 0 if not\n",
    "clean_df = clean_df.withColumn('is_holiday', when(date_format(clean_df['tpepPickupDateTime'], 'yyyy-MM-dd').isin(holiday_dates), 1).otherwise(0))\n",
    "\n",
    "# Indicate hour of day, day of week, and week number of year\n",
    "clean_df = clean_df.withColumn(\"hour_of_day\", F.hour(\"tpepPickupDateTime\"))\n",
    "clean_df = clean_df.withColumn(\"day_of_week\", F.dayofweek(\"tpepPickupDateTime\"))\n",
    "# clean_df = clean_df.withColumn('week_number', weekofyear(clean_df['tpepPickupDateTime']))\n",
    "\n",
    "# weekofyear() function was not calculating week number correctly so created our own function\n",
    "# Define a UDF to calculate the week number\n",
    "def custom_week_number(date_time):\n",
    "    # Get the year of the date\n",
    "    year = date_time.year\n",
    "    # Get the first day of the year\n",
    "    first_day_of_year = datetime.datetime(year, 1, 1)\n",
    "    # Calculate the day of the week for January 1st\n",
    "    jan_1_day_of_week = first_day_of_year.isoweekday()\n",
    "    # Get the current date's day of the week\n",
    "    current_day_of_week = date_time.isoweekday()\n",
    "    # Calculate the week number\n",
    "    if jan_1_day_of_week == 7:  # If January 1st is Sunday\n",
    "        week_number = (date_time - first_day_of_year).days // 7 + 1\n",
    "    else:\n",
    "        week_number = (date_time - first_day_of_year).days // 7 + 1\n",
    "        if week_number == 0:\n",
    "            week_number = 52\n",
    "    # Adjust for the case where December 31st falls on a Sunday or calculated as week number 53\n",
    "    if date_time.month == 12 and date_time.day == 31:\n",
    "        if current_day_of_week == 7 or week_number == 53:\n",
    "            week_number = 52\n",
    "    return week_number\n",
    "\n",
    "# Register the UDF\n",
    "week_number_udf = udf(custom_week_number, IntegerType())\n",
    "\n",
    "# Apply the UDF to calculate the week number\n",
    "clean_df = clean_df.withColumn('week_number', week_number_udf(clean_df['tpepPickupDateTime']))\n",
    "\n",
    "# Now, calculate demand of taxis for each hour of day by zone\n",
    "# Convert tpepPickupDateTime to a timestamp data type\n",
    "clean_df = clean_df.withColumn(\"tpepPickupDateTime\", F.to_timestamp(\"tpepPickupDateTime\"))\n",
    "\n",
    "# Aggregate the df by date, hour, and puLocationId\n",
    "# Get the data in desired format\n",
    "aggregated_df = clean_df.groupby(\n",
    "    F.concat(\n",
    "        F.date_format(\"tpepPickupDateTime\", \"yyyy-MM-dd\"),\n",
    "        F.lit(\" \"),\n",
    "        F.format_string(\"%02d\", F.col(\"hour_of_day\"))  # Ensure hour is zero-padded\n",
    "    ).alias(\"Date_Hour\"),\n",
    "    \"puLocationId\"\n",
    ").agg(\n",
    "    F.first(\"day_of_week\").alias(\"day_of_week\"),\n",
    "    F.first(\"week_number\").alias(\"week_number\"),\n",
    "    F.first(\"is_weekend\").alias(\"is_weekend\"),\n",
    "    F.first(\"is_holiday\").alias(\"is_holiday\"),\n",
    "    F.count(\"*\").alias(\"demand\")\n",
    ")\n",
    "\n",
    "# Convert the Date_Hour column to timestamp\n",
    "aggregated_df = aggregated_df.withColumn(\"Date_Hour\", F.to_timestamp(\"Date_Hour\", \"yyyy-MM-dd HH\"))\n",
    "\n",
    "# Order the df by Date_Hour in ascending order\n",
    "aggregated_df = aggregated_df.orderBy(asc(\"Date_Hour\"))\n",
    "\n",
    "# aggregated_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dc07cdc-011f-4f41-b436-ea5106dc5d26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Initial Time-Series Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40710cac-876a-48f3-9e1b-38e1834bfb1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4bd659b-6768-4a6a-be6f-d5b9caa44f9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e44d1f4-3004-4822-a684-22a576f1b79c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Initial Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "692139f1-7e57-40ff-93e8-60ad204210bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fine-tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de92d288-826a-4f30-b02f-b3e6994218e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [238.426160063352,106.55735860893428,201.491767555364,91.67937709709865,138.71006049308804,56.60703785537175,29.413271085132916,7.21237083188831,59.26370286259451,128.19654798434053,205.78521877804621,-5.3890808904700815,33.348882651236345,-33.982129555074074,289.34349135542345,246.9365115109923,164.6667961669405,133.68981566822185,217.50030787829579,249.10698945806973,78.6038526101135,237.366379575261,70.70500888412437,282.914030913867,323.5354443608307,86.43412942346752,161.75532714397707,29.75053307236997,69.08896186194691,214.99830204386015,-28.61178158972116,132.44745428180735,-92.15247301418086,-57.668807772686435,75.00841046217161,-75.40219742330038,-92.42938019938744,-4.245808375381753,6.657587681664653,-65.63999841027601,-49.550942508159885,-39.36918561781746,-100.03925256612378,-8.417078620063608,-105.89857176730409,0.9136261598478735,27.156059248446976,-115.94266016787492,-121.07201771018923,-90.45627816627687,-68.6864724187703,-113.25423842361279,-28.613995350945512,-103.1237347896437,-122.96901537841102,-108.32374858090353,-123.18578783374146,-112.7103879525057,-131.82336321431237,-128.12216720745025,-132.78098560724533,-133.41609608944322,-132.76450356476428,-134.5641630575487,-134.94411546359123,-132.58162313041603,-132.48136141345614,-133.01907951368472,5.004539859656276,-0.26374506875948767,-4.250203923260317,-43.09225213091208]\nIntercept: 122.44809361063707\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Date_Hour' to date type\n",
    "aggregated_df = aggregated_df.withColumn(\"Date\", to_date(\"Date_Hour\"))\n",
    "\n",
    "# Split the data into training and testing sets based on the date\n",
    "train_data = aggregated_df.filter((col('Date') >= lit('2017-01-01')) & (col('Date') <= lit('2018-05-31')))\n",
    "test_data = aggregated_df.filter((col('Date') >= lit('2018-06-01')) & (col('Date') <= lit('2018-12-31')))\n",
    "\n",
    "# Define the stages for the pipeline\n",
    "string_indexer = StringIndexer(inputCol=\"puLocationId\", outputCol=\"puLocationId_Indexed\", handleInvalid=\"skip\")\n",
    "encoder = OneHotEncoder(inputCols=[\"puLocationId_Indexed\"], outputCols=[\"puLocationId_OHE\"])\n",
    "assembler = VectorAssembler(inputCols=[\"puLocationId_OHE\", \"day_of_week\", \"week_number\", \"is_weekend\", \"is_holiday\"], outputCol=\"features\")\n",
    "# Add regularization to the model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"demand\", elasticNetParam=0.8, regParam=0.1)\n",
    "\n",
    "# Create the pipeline with the new regularized linear regression\n",
    "pipeline = Pipeline(stages=[string_indexer, encoder, assembler, lr])\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])  # 0.0 = L2, 1.0 = L1\n",
    "             .addGrid(lr.regParam, [0.1, 0.01, 0.001])\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    evaluator=RegressionEvaluator(labelCol=\"demand\"),\n",
    "                    numFolds=3)\n",
    "\n",
    "# Run cross validations. This step can take some time to compute.\n",
    "cvModel = cv.fit(train_data)\n",
    "\n",
    "# Look at the model coefficients and intercept\n",
    "bestModel = cvModel.bestModel.stages[-1]\n",
    "print(f\"Coefficients: {bestModel.coefficients}\")\n",
    "print(f\"Intercept: {bestModel.intercept}\")\n",
    "\n",
    "# Evaluate best model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d131a2f0-1c26-4662-a258-00c6b46ecd8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regParam: 0.001\nBest elasticNetParam: 0.0\nTest RMSE: 118.37501501341444\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"demand\", metricName=\"rmse\")\n",
    "\n",
    "# Get the best regularization parameters\n",
    "bestRegParam = bestModel._java_obj.getRegParam()\n",
    "bestElasticNetParam = bestModel._java_obj.getElasticNetParam()\n",
    "print(f\"Best regParam: {bestRegParam}\")\n",
    "print(f\"Best elasticNetParam: {bestElasticNetParam}\")\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "predictions = cvModel.transform(test_data)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Test RMSE: {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "milestone3_linear_finetune",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
