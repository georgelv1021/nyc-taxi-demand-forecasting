{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc836024-741e-4d4d-bd67-32d64c34b3c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Step 1 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6293277e-9c90-4ec3-a27b-d1407e1ac27b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, expr, dayofweek, weekofyear, date_format, asc, udf, lit, month\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import IntegerType\n",
    "import datetime\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7f37cc0-e4bd-419e-897a-ac921f801285",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify Azure Blob Storage account details\n",
    "storage_account_name = \"qlintaxi\"\n",
    "storage_account_access_key = \"pMMk6yCETTB5NBV4q4HT1Wor8G/oGClcBQ2MR0jBygsW0fb7F5fNfn001nlBj5G7OpIqSh1YSLzm+ASt4NoAwg==\"\n",
    "container_name = \"taxi\"\n",
    "\n",
    "# Configure the Azure Blob Storage account credentials\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\",\n",
    "    storage_account_access_key\n",
    ")\n",
    "\n",
    "# Build the storage file path\n",
    "taxi_file_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/clean_data.parquet\"\n",
    "# Read the Parquet data\n",
    "taxi_df = spark.read.parquet(taxi_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf4bc070-2602-45bd-8299-8461a7735b3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Step 2 Join the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "847e50d0-ba63-4831-848f-2db3050ee25b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the Taxi Zone\n",
    "zone_file_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/taxi-zone-lookup.csv\"\n",
    "\n",
    "# Read the CSV data\n",
    "zone_df = spark.read.csv(zone_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cb2e06f-dcf1-4cba-b815-f2c6ffd7e2ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join filtered weather data with df_2017_Jan_Feb on matching datetime values\n",
    "final_df = taxi_df.join(zone_df, taxi_df.puLocationId == zone_df.LocationID)\n",
    "\n",
    "# Drop the datetime column from the joined_df DataFrame\n",
    "final_df = final_df.drop(\"puLocationId\", \"LocationID\", \"Borough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a174a7f-15d8-4584-83c6-f9a7c0dde887",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, to_timestamp\n",
    "# Convert 'Date_Hour' from string to timestamp if it's not already\n",
    "final_df = final_df.withColumn('Date_Hour', to_timestamp(col('Date_Hour'), 'yyyy-MM-dd\\'T\\'HH:mm:ss.SSSXXX'))\n",
    "\n",
    "# Extract year, month, day, and hour from 'Date_Hour'\n",
    "final_df = final_df.withColumn('Year', year(col('Date_Hour')))\n",
    "final_df = final_df.withColumn('Month', month(col('Date_Hour')))\n",
    "final_df = final_df.withColumn('Day', dayofmonth(col('Date_Hour')))\n",
    "final_df = final_df.withColumn('Hour', hour(col('Date_Hour')))\n",
    "\n",
    "# Sort the DataFrame by Date_Hour in ascending order\n",
    "final_df = final_df.orderBy('Date_Hour')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40546d71-cc94-4a83-93cf-4ad73b7de7d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9726b2de-fa72-4d46-998a-55a5867e6932",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Step 3 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4f83963-82fc-402b-b2bc-788cd2a7b0b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3.1 Train/Valid/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "753060a1-c595-41e4-8566-881f49dc8a29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure the DataFrame is ordered by time\n",
    "final_df = final_df.orderBy(\"Date_Hour\")\n",
    "\n",
    "# Calculate split dates\n",
    "total_rows = final_df.count()\n",
    "train_split_index = int(total_rows * 0.7)\n",
    "val_split_index = train_split_index + int(total_rows * 0.15)\n",
    "\n",
    "# Use Window function to assign row numbers\n",
    "windowSpec = Window.orderBy(\"Date_Hour\")\n",
    "final_df = final_df.withColumn(\"row_number\", F.row_number().over(windowSpec))\n",
    "\n",
    "# Find the dates corresponding to split indices\n",
    "train_split_date = final_df.where(col(\"row_number\") == train_split_index).select(\"Date_Hour\").collect()[0][\"Date_Hour\"]\n",
    "val_split_date = final_df.where(col(\"row_number\") == val_split_index).select(\"Date_Hour\").collect()[0][\"Date_Hour\"]\n",
    "\n",
    "# Remove the temporary column\n",
    "final_df = final_df.drop(\"row_number\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "647d0c7c-690e-41f0-bfc2-3f3f30913d3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_data = final_df.where(col(\"Date_Hour\") <= train_split_date)\n",
    "validation_data = final_df.where((col(\"Date_Hour\") > train_split_date) & (col(\"Date_Hour\") <= val_split_date))\n",
    "test_data = final_df.where(col(\"Date_Hour\") > val_split_date)\n",
    "\n",
    "# Verify the splits by checking the min and max dates in each DataFrame\n",
    "# train_data.select(F.min(\"Date_Hour\"), F.max(\"Date_Hour\")).show()\n",
    "# validation_data.select(F.min(\"Date_Hour\"), F.max(\"Date_Hour\")).show()\n",
    "# test_data.select(F.min(\"Date_Hour\"), F.max(\"Date_Hour\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb7122a6-97a8-44d5-b85b-c636f4f38e2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Indexing the 'Zone' column\n",
    "zone_indexer = StringIndexer(inputCol='Zone', outputCol='ZoneIndexed')\n",
    "\n",
    "# One-hot encoding the indexed 'Zone' column (optional depending on the model and size of the dataset)\n",
    "zone_encoder = OneHotEncoder(inputCols=['ZoneIndexed'], outputCols=['ZoneVec'])\n",
    "\n",
    "# Update the assembler with the new columns\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['Year', 'Month', 'Day', 'Hour', 'day_of_week', 'week_number', 'is_weekend', 'is_holiday', 'ZoneVec'],\n",
    "    outputCol='features'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0642bf0-541c-4559-9381-91258bfd2ee2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3.2 XGBoost with Hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "018e828f-808b-46a2-ba6c-3379bdedae71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 1: Define the Search Space for Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ce204b8-46aa-4916-8b5f-8939d73def33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, SparkTrials\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import numpy as np\n",
    "\n",
    "def get_or_create_spark_session():\n",
    "    return SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Define the search space\n",
    "space = {\n",
    "    'max_depth': hp.quniform('max_depth', 3, 10, 1),  # Clear range, requires rounding\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),  # Correctly specified log-uniform\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),  # Continuous, as it makes sense for ratios\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),  # Continuous, same reason\n",
    "    'gamma': hp.uniform('gamma', 0, 5),  # Continuous, assuming a wide exploration is needed\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 1000, 1),  # Range with rounding, for clarity\n",
    "    'min_child_weight': hp.uniform('min_child_weight', 1, 10)  # Continuous, for more nuanced control\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "269d536b-348a-40e6-af16-5cae6976c1f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 2: Define the Objective Function for Hyperop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3457e1a9-ed15-4fe1-9e3d-c898f1f5a475",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "def objective(hyperparams):\n",
    "    # Ensure MLflow is tracking to the correct experiment\n",
    "    mlflow.set_experiment('/Users/qifan.lin@vanderbilt.edu/XGBoost Tuning')\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Log only the hyperparameters being tested\n",
    "        mlflow.log_params(hyperparams)\n",
    "        \n",
    "        # Create and train the model\n",
    "        xgbRegressor = SparkXGBRegressor(\n",
    "            features_col=\"features\",\n",
    "            label_col=\"demand\",\n",
    "            max_depth=int(hyperparams['max_depth']),\n",
    "            learning_rate=hyperparams['learning_rate'],\n",
    "            subsample=hyperparams['subsample'],\n",
    "            colsample_bytree=hyperparams['colsample_bytree'],\n",
    "            gamma = hyperparams['gamma'],\n",
    "            n_estimators = int(hyperparams['n_estimators']),\n",
    "            min_child_weight = hyperparams['min_child_weight'],\n",
    "            verbosity=0,\n",
    "            objective='reg:squarederror'    \n",
    "        )\n",
    "       \n",
    "       # Use the stages from before, just replace the XGBRegressor with new params\n",
    "        pipeline = Pipeline(stages=[zone_indexer, zone_encoder, assembler, xgbRegressor])\n",
    "    \n",
    "        # Train the model\n",
    "        model = pipeline.fit(train_data)\n",
    "        \n",
    "        # Evaluate the model on the validation set\n",
    "        val_predictions = model.transform(validation_data)\n",
    "        evaluator = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        val_rmse = evaluator.evaluate(val_predictions)\n",
    "        \n",
    "        # Log the key metric\n",
    "        mlflow.log_metric(\"validation_rmse\", val_rmse)\n",
    "\n",
    "    return {'loss': val_rmse, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f912cc5b-06c9-4f5e-b074-2ca71e2efbbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 3: Run Hyperopt for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6ca6f6d-8e5f-48b6-9967-a0e2522b69cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment('/Users/qifan.lin@vanderbilt.edu/XGBoost Tuning')\n",
    "\n",
    "trials = Trials()\n",
    "# Start a parent run for the hyperparameter tuning process\n",
    "with mlflow.start_run(run_name='Hyperparameter Tuning Parent Run'):\n",
    "    best_hyperparams = fmin(fn=objective, \n",
    "                            space=space, \n",
    "                            algo=tpe.suggest, \n",
    "                            max_evals=128, \n",
    "                            trials=trials)\n",
    "    mlflow.log_params(best_hyperparams)  # Optionally log the best hyperparameters at the parent run level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ad46ea5-c613-4dd4-a621-79175872a1e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5899287e-a99b-4759-9134-93c4e938e3fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 4: Train the Final Model and Evaluate on Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e606f1c-618e-4b40-8f55-9d946a8de5e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Combine training and validation sets for final model training\n",
    "final_train_data = train_data.union(validation_data)\n",
    "\n",
    "# Train the final model using the best hyperparameters found\n",
    "best_hyperparams = {\n",
    "    'max_depth': int(best_hyperparams['max_depth']),\n",
    "    'learning_rate': best_hyperparams['learning_rate'],\n",
    "    'subsample': best_hyperparams['subsample'],\n",
    "    'colsample_bytree': best_hyperparams['colsample_bytree'],\n",
    "    'gamma': best_hyperparams['gamma'],\n",
    "    'n_estimators': int(best_hyperparams['n_estimators']),\n",
    "    'min_child_weight': best_hyperparams['min_child_weight'],\n",
    "}\n",
    "\n",
    "# Re-create the model with the best hyperparameters\n",
    "final_xgbRegressor = SparkXGBRegressor(\n",
    "    features_col=\"features\",\n",
    "    label_col=\"demand\",\n",
    "    maxDepth=best_hyperparams['max_depth'],\n",
    "    learning_rate=best_hyperparams['learning_rate'],\n",
    "    subsample=best_hyperparams['subsample'],\n",
    "    colsampleBytree=best_hyperparams['colsample_bytree'],\n",
    "    gamma=best_hyperparams['gamma'],\n",
    "    numRound=best_hyperparams['n_estimators'],\n",
    "    minChildWeight=best_hyperparams['min_child_weight'],\n",
    "    objective='reg:squarederror'\n",
    ")\n",
    "\n",
    "final_pipeline = Pipeline(stages=[zone_indexer, zone_encoder, assembler, final_xgbRegressor])\n",
    "\n",
    "# Train the final model on the combined training and validation dataset\n",
    "final_model = final_pipeline.fit(final_train_data)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "test_predictions = final_model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "print(\"Test RMSE:\", test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50a61f84-fb51-431c-b476-8c56d34aa358",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Step 4 Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8bbc446-c6e3-4c36-8a53-f0a4c6cef704",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "rm -rf /dbfs/tmp/xgboost/pipeline_001\n",
    "rm -rf /dbfs/tmp/xgboost/pipelineModel_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2af85131-648e-4c49-8243-5e051607b13c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the pipeline that created the model\n",
    "final_pipeline.save('/tmp/xgboost/pipeline_001')\n",
    " \n",
    "# Save the model itself\n",
    "final_model.save('/tmp/xgboost/pipelineModel_001')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Modeling_XGBoost",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
