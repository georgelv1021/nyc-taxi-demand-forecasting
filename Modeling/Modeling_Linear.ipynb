{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc836024-741e-4d4d-bd67-32d64c34b3c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Step 1 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6293277e-9c90-4ec3-a27b-d1407e1ac27b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, expr, dayofweek, weekofyear, date_format, asc, udf, lit, month\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import IntegerType\n",
    "import datetime\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f37cc0-e4bd-419e-897a-ac921f801285",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify Azure Blob Storage account details\n",
    "storage_account_name = \"qlintaxi\"\n",
    "storage_account_access_key = \"pMMk6yCETTB5NBV4q4HT1Wor8G/oGClcBQ2MR0jBygsW0fb7F5fNfn001nlBj5G7OpIqSh1YSLzm+ASt4NoAwg==\"\n",
    "container_name = \"taxi\"\n",
    "\n",
    "# Configure the Azure Blob Storage account credentials\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\",\n",
    "    storage_account_access_key\n",
    ")\n",
    "\n",
    "# Build the storage file path\n",
    "taxi_file_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/clean_data.parquet\"\n",
    "# Read the Parquet data\n",
    "taxi_df = spark.read.parquet(taxi_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d28eb16-bdd1-46c1-91cf-843c741741a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(taxi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf4bc070-2602-45bd-8299-8461a7735b3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Step 2 Join the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "847e50d0-ba63-4831-848f-2db3050ee25b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the Taxi Zone\n",
    "zone_file_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/taxi-zone-lookup.csv\"\n",
    "\n",
    "# Read the CSV data\n",
    "zone_df = spark.read.csv(zone_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cb2e06f-dcf1-4cba-b815-f2c6ffd7e2ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join filtered weather data with df_2017_Jan_Feb on matching datetime values\n",
    "final_df = taxi_df.join(zone_df, taxi_df.puLocationId == zone_df.LocationID)\n",
    "\n",
    "# Drop the datetime column from the joined_df DataFrame\n",
    "final_df = final_df.drop(\"puLocationId\", \"LocationID\", \"Borough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a174a7f-15d8-4584-83c6-f9a7c0dde887",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, to_timestamp\n",
    "# Convert 'Date_Hour' from string to timestamp if it's not already\n",
    "final_df = final_df.withColumn('Date_Hour', to_timestamp(col('Date_Hour'), 'yyyy-MM-dd\\'T\\'HH:mm:ss.SSSXXX'))\n",
    "\n",
    "# Extract year, month, day, and hour from 'Date_Hour'\n",
    "final_df = final_df.withColumn('Year', year(col('Date_Hour')))\n",
    "final_df = final_df.withColumn('Month', month(col('Date_Hour')))\n",
    "final_df = final_df.withColumn('Day', dayofmonth(col('Date_Hour')))\n",
    "final_df = final_df.withColumn('Hour', hour(col('Date_Hour')))\n",
    "\n",
    "# Sort the DataFrame by Date_Hour in ascending order\n",
    "final_df = final_df.orderBy('Date_Hour')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40546d71-cc94-4a83-93cf-4ad73b7de7d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9726b2de-fa72-4d46-998a-55a5867e6932",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Step 3 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4f83963-82fc-402b-b2bc-788cd2a7b0b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3.1 Train/Valid/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "753060a1-c595-41e4-8566-881f49dc8a29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure the DataFrame is ordered by time\n",
    "final_df = final_df.orderBy(\"Date_Hour\")\n",
    "\n",
    "# Calculate split dates\n",
    "total_rows = final_df.count()\n",
    "train_split_index = int(total_rows * 0.7)\n",
    "val_split_index = train_split_index + int(total_rows * 0.15)\n",
    "\n",
    "# Use Window function to assign row numbers\n",
    "windowSpec = Window.orderBy(\"Date_Hour\")\n",
    "final_df = final_df.withColumn(\"row_number\", F.row_number().over(windowSpec))\n",
    "\n",
    "# Find the dates corresponding to split indices\n",
    "train_split_date = final_df.where(col(\"row_number\") == train_split_index).select(\"Date_Hour\").collect()[0][\"Date_Hour\"]\n",
    "val_split_date = final_df.where(col(\"row_number\") == val_split_index).select(\"Date_Hour\").collect()[0][\"Date_Hour\"]\n",
    "\n",
    "# Remove the temporary column\n",
    "final_df = final_df.drop(\"row_number\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "647d0c7c-690e-41f0-bfc2-3f3f30913d3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_data = final_df.where(col(\"Date_Hour\") <= train_split_date)\n",
    "validation_data = final_df.where((col(\"Date_Hour\") > train_split_date) & (col(\"Date_Hour\") <= val_split_date))\n",
    "test_data = final_df.where(col(\"Date_Hour\") > val_split_date)\n",
    "\n",
    "# Verify the splits by checking the min and max dates in each DataFrame\n",
    "# train_data.select(F.min(\"Date_Hour\"), F.max(\"Date_Hour\")).show()\n",
    "# validation_data.select(F.min(\"Date_Hour\"), F.max(\"Date_Hour\")).show()\n",
    "# test_data.select(F.min(\"Date_Hour\"), F.max(\"Date_Hour\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7122a6-97a8-44d5-b85b-c636f4f38e2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Indexing the 'Zone' column\n",
    "zone_indexer = StringIndexer(inputCol='Zone', outputCol='ZoneIndexed')\n",
    "\n",
    "# One-hot encoding the indexed 'Zone' column (optional depending on the model and size of the dataset)\n",
    "zone_encoder = OneHotEncoder(inputCols=['ZoneIndexed'], outputCols=['ZoneVec'])\n",
    "\n",
    "# Update the assembler with the new columns\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['Year', 'Month', 'Day', 'Hour', 'day_of_week', 'week_number', 'is_weekend', 'is_holiday', 'ZoneVec'],\n",
    "    outputCol='features'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0642bf0-541c-4559-9381-91258bfd2ee2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3.2 Linear Regression with Hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "018e828f-808b-46a2-ba6c-3379bdedae71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 1: Define the Search Space for Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ce204b8-46aa-4916-8b5f-8939d73def33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, SparkTrials\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import numpy as np\n",
    "\n",
    "def get_or_create_spark_session():\n",
    "    return SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Define the search space\n",
    "space = {\n",
    "    'regParam': hp.uniform('regParam', 0.01, 0.1),  # Regularization parameter\n",
    "    'elasticNetParam': hp.uniform('elasticNetParam', 0, 1),  # Elastic net mixing parameter\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "269d536b-348a-40e6-af16-5cae6976c1f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 2: Define the Objective Function for Hyperop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3457e1a9-ed15-4fe1-9e3d-c898f1f5a475",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "def objective(hyperparams):\n",
    "    # Ensure MLflow is tracking to the correct experiment\n",
    "    mlflow.set_experiment('/Users/zhaozhou.lyu@vanderbilt.edu/Linear_Tuning')\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Log only the hyperparameters being tested\n",
    "        mlflow.log_params(hyperparams)\n",
    "        \n",
    "        # Create and train the Random Forest model\n",
    "        lrRegressor = LinearRegression(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"demand\",\n",
    "            regParam=hyperparams['regParam'],\n",
    "            elasticNetParam=hyperparams['elasticNetParam'],\n",
    "            # Add other hyperparameters here, such as 'fitIntercept', 'standardization', if needed\n",
    "        )\n",
    "       \n",
    "       # Use the stages from before, just replace the XGBRegressor with new params\n",
    "        pipeline = Pipeline(stages=[zone_indexer, zone_encoder, assembler, lrRegressor])\n",
    "    \n",
    "        # Train the model\n",
    "        model = pipeline.fit(train_data)\n",
    "        \n",
    "        # Evaluate the model on the validation set\n",
    "        val_predictions = model.transform(validation_data)\n",
    "        evaluator = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        val_rmse = evaluator.evaluate(val_predictions)\n",
    "        \n",
    "        # Log the key metric\n",
    "        mlflow.log_metric(\"validation_rmse\", val_rmse)\n",
    "\n",
    "    return {'loss': val_rmse, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f912cc5b-06c9-4f5e-b074-2ca71e2efbbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 3: Run Hyperopt for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6ca6f6d-8e5f-48b6-9967-a0e2522b69cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/10 21:22:23 INFO mlflow.tracking.fluent: Experiment with name '/Users/zhaozhou.lyu@vanderbilt.edu/RF_Tuning' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/128 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exception while intercepting server message: {'jsonrpc': '2.0', 'id': '18c1c553-f940-47f8-90d8-554a347b9a60', 'method': 'textDocument/codeAction', 'params': {'textDocument': {'uri': '/notebook/2154839837908356'}, 'range': {'start': {'line': 214, 'character': 0}, 'end': {'line': 214, 'character': 2}}, 'context': {'diagnostics': [{'range': {'start': {'line': 214, 'character': 0}, 'end': {'line': 214, 'character': 2}}, 'message': 'Undefined name `rm`', 'code': 'F821', 'data': {'codeLine': '#rm -rf /dbfs/tmp/xgboost/pipeline_001'}}]}}}\nTraceback (most recent call last):\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/base.py\", line 36, in intercept_message_to_server_safe\n    return self.intercept_message_to_server(msg)\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/import_quickfix/import_quickfix.py\", line 43, in intercept_message_to_server\n    word = self._get_name(diagnostic)\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/import_quickfix/import_quickfix.py\", line 70, in _get_name\n    return word_node.value\nAttributeError: 'NoneType' object has no attribute 'value'\n\nexception while intercepting server message: {'jsonrpc': '2.0', 'id': '18c1c553-f940-47f8-90d8-554a347b9a60', 'method': 'textDocument/codeAction', 'params': {'textDocument': {'uri': '/notebook/2154839837908356'}, 'range': {'start': {'line': 214, 'character': 0}, 'end': {'line': 214, 'character': 2}}, 'context': {'diagnostics': [{'range': {'start': {'line': 214, 'character': 0}, 'end': {'line': 214, 'character': 2}}, 'message': 'Undefined name `rm`', 'code': 'F821', 'data': {'codeLine': '#rm -rf /dbfs/tmp/xgboost/pipeline_001'}}]}}}\nTraceback (most recent call last):\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/base.py\", line 36, in intercept_message_to_server_safe\n    return self.intercept_message_to_server(msg)\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/import_quickfix/import_quickfix.py\", line 43, in intercept_message_to_server\n    word = self._get_name(diagnostic)\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/import_quickfix/import_quickfix.py\", line 70, in _get_name\n    return word_node.value\nAttributeError: 'NoneType' object has no attribute 'value'\n\nexception while intercepting server message: {'jsonrpc': '2.0', 'id': '18c1c553-f940-47f8-90d8-554a347b9a60', 'method': 'textDocument/codeAction', 'params': {'textDocument': {'uri': '/notebook/2154839837908356'}, 'range': {'start': {'line': 214, 'character': 0}, 'end': {'line': 214, 'character': 2}}, 'context': {'diagnostics': [{'range': {'start': {'line': 214, 'character': 0}, 'end': {'line': 214, 'character': 2}}, 'message': 'Undefined name `rm`', 'code': 'F821', 'data': {'codeLine': '#rm -rf /dbfs/tmp/xgboost/pipeline_001'}}]}}}\nTraceback (most recent call last):\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/base.py\", line 36, in intercept_message_to_server_safe\n    return self.intercept_message_to_server(msg)\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/import_quickfix/import_quickfix.py\", line 43, in intercept_message_to_server\n    word = self._get_name(diagnostic)\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/import_quickfix/import_quickfix.py\", line 70, in _get_name\n    return word_node.value\nAttributeError: 'NoneType' object has no attribute 'value'\n\nexception while intercepting server message: {'jsonrpc': '2.0', 'id': '2268721f-9c44-49c5-9822-122069ab6fe2', 'method': 'textDocument/codeAction', 'params': {'textDocument': {'uri': '/notebook/2154839837908356'}, 'range': {'start': {'line': 215, 'character': 0}, 'end': {'line': 215, 'character': 2}}, 'context': {'diagnostics': [{'range': {'start': {'line': 215, 'character': 0}, 'end': {'line': 215, 'character': 2}}, 'message': 'Undefined name `rm`', 'code': 'F821', 'data': {'codeLine': '#rm -rf /dbfs/tmp/xgboost/pipelineModel_001'}}]}}}\nTraceback (most recent call last):\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/base.py\", line 36, in intercept_message_to_server_safe\n    return self.intercept_message_to_server(msg)\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/import_quickfix/import_quickfix.py\", line 43, in intercept_message_to_server\n    word = self._get_name(diagnostic)\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/import_quickfix/import_quickfix.py\", line 70, in _get_name\n    return word_node.value\nAttributeError: 'NoneType' object has no attribute 'value'\n\nexception while intercepting server message: {'jsonrpc': '2.0', 'id': '2268721f-9c44-49c5-9822-122069ab6fe2', 'method': 'textDocument/codeAction', 'params': {'textDocument': {'uri': '/notebook/2154839837908356'}, 'range': {'start': {'line': 215, 'character': 0}, 'end': {'line': 215, 'character': 2}}, 'context': {'diagnostics': [{'range': {'start': {'line': 215, 'character': 0}, 'end': {'line': 215, 'character': 2}}, 'message': 'Undefined name `rm`', 'code': 'F821', 'data': {'codeLine': '#rm -rf /dbfs/tmp/xgboost/pipelineModel_001'}}]}}}\nTraceback (most recent call last):\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/base.py\", line 36, in intercept_message_to_server_safe\n    return self.intercept_message_to_server(msg)\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/import_quickfix/import_quickfix.py\", line 43, in intercept_message_to_server\n    word = self._get_name(diagnostic)\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/import_quickfix/import_quickfix.py\", line 70, in _get_name\n    return word_node.value\nAttributeError: 'NoneType' object has no attribute 'value'\n\nexception while intercepting server message: {'jsonrpc': '2.0', 'id': '2268721f-9c44-49c5-9822-122069ab6fe2', 'method': 'textDocument/codeAction', 'params': {'textDocument': {'uri': '/notebook/2154839837908356'}, 'range': {'start': {'line': 215, 'character': 0}, 'end': {'line': 215, 'character': 2}}, 'context': {'diagnostics': [{'range': {'start': {'line': 215, 'character': 0}, 'end': {'line': 215, 'character': 2}}, 'message': 'Undefined name `rm`', 'code': 'F821', 'data': {'codeLine': '#rm -rf /dbfs/tmp/xgboost/pipelineModel_001'}}]}}}\nTraceback (most recent call last):\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/base.py\", line 36, in intercept_message_to_server_safe\n    return self.intercept_message_to_server(msg)\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/import_quickfix/import_quickfix.py\", line 43, in intercept_message_to_server\n    word = self._get_name(diagnostic)\n  File \"/databricks/python_shell/dbruntime/lsp_backend/middleware/import_quickfix/import_quickfix.py\", line 70, in _get_name\n    return word_node.value\nAttributeError: 'NoneType' object has no attribute 'value'\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment('/Users/zhaozhou.lyu@vanderbilt.edu/Linear_Tuning')\n",
    "\n",
    "trials = Trials()\n",
    "# Start a parent run for the hyperparameter tuning process\n",
    "with mlflow.start_run(run_name='Hyperparameter Tuning Parent Run'):\n",
    "    best_hyperparams = fmin(fn=objective, \n",
    "                            space=space, \n",
    "                            algo=tpe.suggest, \n",
    "                            max_evals=64, \n",
    "                            trials=trials)\n",
    "    mlflow.log_params(best_hyperparams)  # Optionally log the best hyperparameters at the parent run level\n",
    "    mlflow.log_param('regParam', float(best_hyperparams['regParam']))\n",
    "    mlflow.log_param('elasticNetParam', float(best_hyperparams['elasticNetParam']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ad46ea5-c613-4dd4-a621-79175872a1e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5899287e-a99b-4759-9134-93c4e938e3fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Step 4: Train the Final Model and Evaluate on Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e606f1c-618e-4b40-8f55-9d946a8de5e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Combine training and validation sets for final model training\n",
    "final_train_data = train_data.union(validation_data)\n",
    "\n",
    "# Train the final model using the best hyperparameters found\n",
    "best_hyperparams_lr = {\n",
    "    'regParam': best_hyperparams['regParam'],\n",
    "    'elasticNetParam': best_hyperparams['elasticNetParam'],\n",
    "}\n",
    "\n",
    "# Re-create the model with the best hyperparameters\n",
    "final_lrRegressor = LinearRegression(\n",
    "    featuresCol=\"features\",  # Correct parameter name\n",
    "    labelCol=\"demand\",  # Correct parameter name\n",
    "    regParam=best_hyperparams_lr['regParam'],\n",
    "    elasticNetParam=best_hyperparams_lr['elasticNetParam'],\n",
    ")\n",
    "\n",
    "final_pipeline = Pipeline(stages=[zone_indexer, zone_encoder, assembler, final_lrRegressor])\n",
    "\n",
    "# Train the final model on the combined training and validation dataset\n",
    "final_model = final_pipeline.fit(final_train_data)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "test_predictions = final_model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(labelCol=\"demand\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "print(\"Test RMSE:\", test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50a61f84-fb51-431c-b476-8c56d34aa358",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Step 4 Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8bbc446-c6e3-4c36-8a53-f0a4c6cef704",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%sh\n",
    "#rm -rf /dbfs/tmp/xgboost/pipeline_001\n",
    "#rm -rf /dbfs/tmp/xgboost/pipelineModel_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2af85131-648e-4c49-8243-5e051607b13c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the pipeline that created the model\n",
    "final_pipeline.save('pipeline_lr')\n",
    " \n",
    "# Save the model itself\n",
    "final_model.save('lrfinetone_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e286fd24-71f6-402a-80fb-2c01502ff14c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Modeling_Linear",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
