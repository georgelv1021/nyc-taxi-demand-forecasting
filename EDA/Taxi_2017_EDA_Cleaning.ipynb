{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03717f3-eaa8-445b-a5b1-fa36d8e4dcff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# EDA and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5befddb3-2cb4-4adf-a259-f7a165e6ba3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fda7201-ba2f-4c4f-9292-b27b6bd5a684",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RUN\n",
    "# import libraries needed\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, expr, dayofweek, date_format, sum\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import weekofyear\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import TimestampType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc68099d-34b4-4b6f-ab9a-1aa8b6fd5945",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RUN\n",
    "# create spark session\n",
    "spark = SparkSession.builder.appName(\"FinalProject\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb23fc76-bcb9-4a7c-a208-62f8fd6f6f7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RUN\n",
    "# data : https://learn.microsoft.com/en-us/azure/open-datasets/dataset-taxi-yellow?tabs=pyspark#azure-databricks\n",
    "\n",
    "# Azure storage access info\n",
    "blob_account_name = \"azureopendatastorage\"\n",
    "blob_container_name = \"nyctlc\"\n",
    "blob_relative_path = \"yellow\"\n",
    "blob_sas_token = \"r\"\n",
    "\n",
    "# Allow SPARK to read from Blob remotely\n",
    "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
    "spark.conf.set(\n",
    "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
    "  blob_sas_token)\n",
    "print('Remote blob path: ' + wasbs_path)\n",
    "\n",
    "# SPARK read parquet, note that it won't load any data yet by now\n",
    "df = spark.read.parquet(wasbs_path)\n",
    "#print('Register the DataFrame as a SQL temporary view: source')\n",
    "#df.createOrReplaceTempView('source')\n",
    "\n",
    "# Display top 10 rows\n",
    "#print('Displaying top 10 rows: ')\n",
    "#display(spark.sql('SELECT * FROM source LIMIT 10'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d351d4aa-37fb-46d6-b98f-5418169727b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f40f0259-743e-451a-a348-61356f3bc3e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# smaller dataset bc data is big. Will use the truncated df for the project\n",
    "df_2017 = df.filter((col(\"puYear\") == 2017))\n",
    "\n",
    "# drop unneccessar columns\n",
    "df_2017 = df_2017.drop(\"vendorID\", \"rateCodeId\", \"storeAndFwdFlag\",\n",
    "                       \"paymentType\", \"extra\", \"mtaTax\", \"improvementSurcharge\", \n",
    "                       \"tipAmount\", \"tollsAmount\", \"fareAmount\", \n",
    "                       \"startLon\", \"startLat\", \"endLon\", \"endLat\")\n",
    "\n",
    "#display(df_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "205f79af-2e56-43b4-9552-aa518ca71f41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- New York City includes different boroughs, Bronx, Brooklyn, EWR, Queens, Staten Islands, and Manhattan. We want to only focus on Manhattan because that's the busiest borough.\n",
    "- The original dataset from vendor has longitue and latitue infos, however, the data stored in Azure does not. Hence, I mannually filter the Manhattan zones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecd934d7-a7b1-4fcc-b106-993f630d0e6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. NYC Borough Location Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fdab094-2d9d-47c8-b66a-9c7eae6ae2f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the taxi zone csv from the hive metastore\n",
    "taxi_zone = spark.sql(\"SELECT LocationID, Borough, Zone FROM `hive_metastore`.`default`.`taxi_zone_lookup`;\")\n",
    "#display(taxi_zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "322d81f8-7657-4678-a316-720d3c7c3d67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = df_2017.join(taxi_zone, df_2017.puLocationId == taxi_zone.LocationID, 'left_outer')\n",
    "\n",
    "# Drop the LocationID column from df2 after the join\n",
    "joined_df = joined_df.drop(joined_df.LocationID)\n",
    "\n",
    "#display(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70f13538-6ee0-4829-ac70-d9e41cf7d149",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Group by 'Borough' and count the number of occurrences\n",
    "borough_counts = joined_df.groupBy('Borough').count()\n",
    "\n",
    "# Order by count descending\n",
    "borough_counts_ordered = borough_counts.orderBy(F.desc('count'))\n",
    "\n",
    "# Collect the ordered data\n",
    "ordered_data = borough_counts_ordered.collect()\n",
    "\n",
    "# Extract the borough names and counts as lists\n",
    "borough_names = [row.Borough for row in ordered_data]\n",
    "counts = [row['count'] for row in ordered_data]\n",
    "\n",
    "# Calculate total count\n",
    "total_count = borough_counts.agg(F.sum('count')).collect()[0][0]\n",
    "\n",
    "# Create the bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define colors for each borough, assuming borough_names has been sorted or arranged as desired\n",
    "borough_colors = plt.cm.Paired(range(len(borough_names)))  # Adjusted part: use the same color mapping logic\n",
    "\n",
    "bars = plt.bar(borough_names, counts, color=borough_colors)\n",
    "\n",
    "# Add individual counts above bars\n",
    "for i, bar in enumerate(bars):\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{int(yval):,}',ha='center',va='bottom')  # va='bottom' to align the text\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Borough')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Pickups by Borough')\n",
    "\n",
    "# Create a legend showing the total count\n",
    "plt.legend([f'Total Pickups: {int(total_count):,}'], loc='upper right')\n",
    "\n",
    "# Remove the spines\n",
    "ax = plt.gca()  # Get the current axes\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ef9e0ff-ab72-4ea5-9e1c-0a2bf661a008",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Analysis result: Keep only the Manhattan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b3b5d04-c516-4579-b2da-25738cd062d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "\n",
    "# Group by 'Borough' and count the number of occurrences\n",
    "borough_counts = joined_df.groupBy('Borough').count()\n",
    "\n",
    "# Order by count descending\n",
    "borough_counts_ordered = borough_counts.orderBy(F.desc('count'))\n",
    "\n",
    "# Collect the ordered data\n",
    "ordered_data = borough_counts_ordered.collect()\n",
    "\n",
    "# Extract the borough names and counts as lists\n",
    "borough_names = [row.Borough for row in ordered_data]\n",
    "counts = [row['count'] for row in ordered_data]\n",
    "\n",
    "# Calculate total count\n",
    "total_count = borough_counts.agg(F.sum('count')).collect()[0][0]\n",
    "\n",
    "# Define colors for each borough, use a more vibrant color map\n",
    "borough_colors = plt.cm.viridis(np.linspace(0.3, 0.7, len(borough_names)))\n",
    "\n",
    "# Create the bar graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.bar(borough_names, counts, color=borough_colors)\n",
    "\n",
    "# Customize the plot with improved aesthetics\n",
    "plt.xlabel('Borough', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.title('Distribution of Pickups by Borough', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Add gridlines for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add individual counts above bars with improved text properties\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, height, f'{int(height):,}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Set the ticks' properties\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Add the legend with a simplified message\n",
    "plt.legend([f'Total: {total_count:,}'], loc='upper right', fontsize=12)\n",
    "\n",
    "# Remove spines but leave the bottom spine for grounding\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_color('#DDDDDD')  # Light color for bottom spine\n",
    "\n",
    "# Set background color within the plot\n",
    "ax.set_facecolor('#f8f8f8')\n",
    "\n",
    "# Set a frame around the entire plot\n",
    "plt.gcf().set_edgecolor('#f0f0f0')\n",
    "plt.gcf().set_linewidth(1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1be782d1-80cb-4fa6-af3a-e93893161090",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Man_df = joined_df.filter(joined_df[\"Borough\"] == \"Manhattan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c5afc8-62bb-4ad6-8ba8-9a77c0dbd126",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Manhattan Demand Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dde79067-6837-41b4-80fe-641806cd701a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by 'Zone' and count the pickups\n",
    "pickup_counts = Man_df.groupby('Zone').count().select(\"Zone\", col(\"count\").alias(\"counts\"))\n",
    "display(pickup_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94983e3-170e-4bfd-b70c-1a72ccd0391a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read taxi_zones csv from Hive Metastore\n",
    "zones_geom = spark.table(\"default.taxi_zones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b8dd3d-96cd-4fb5-8323-b36dee190891",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Perform the join operation\n",
    "zone_df = zones_geom.join(pickup_counts, zones_geom[\"zone\"] == pickup_counts[\"Zone\"])\n",
    "\n",
    "# Select the desired columns\n",
    "#zone_df = zone_df.select(zone_df[\"Zone\"], col(\"counts\"), col(\"the_geom\"))\n",
    "\n",
    "# Show the result\n",
    "# display(zone_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9707d31-c2e6-41f0-8728-0f7830e0a4bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "zone_df = pickup_counts.join(zones_geom, pickup_counts.Zone == zones_geom.zone, 'left_outer')\n",
    "\n",
    "# Drop the LocationID column from df2 after the join\n",
    "zone_df = zone_df.drop(zone_df[\"borough\"])\n",
    "display(zone_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "126d77d4-0202-4e2b-8060-5946d759db8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Pandas dataframe\n",
    "zone_pandas = zone_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a89da74-500f-4c18-aafc-82511b6763d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "# Convert the_geom column to geometry\n",
    "zone_pandas['geometry'] = zone_pandas['the_geom'].apply(wkt.loads)\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "gdf_zones = gpd.GeoDataFrame(zone_pandas, geometry='geometry', crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c9e3ea-c61e-4865-9817-320a177a9d41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Assuming 'gdf_zones' is your GeoDataFrame\n",
    "\n",
    "# Sort the GeoDataFrame by 'counts' to get the top 10 zones\n",
    "top_zones_10 = gdf_zones.nlargest(10, 'counts')\n",
    "top_zones_5 = gdf_zones.nlargest(5, 'counts')\n",
    "\n",
    "# Plot the map\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10), gridspec_kw={'width_ratios': [3, 1]})\n",
    "\n",
    "gdf_zones.plot(ax=ax1, column='counts', cmap='viridis')\n",
    "\n",
    "# Add title to the map\n",
    "ax1.set_title('Demand by Zone', fontdict={'fontsize': 16}, loc='center')\n",
    "ax1.set_axis_off()\n",
    "\n",
    "# Annotate only the top 10 zones on the map\n",
    "for idx, row in top_zones_5.iterrows():\n",
    "    if row['geometry']:  # This checks if the geometry is not None\n",
    "        ax1.annotate(text=row['Zone'], xy=(row['geometry'].centroid.x, row['geometry'].centroid.y),\n",
    "                     horizontalalignment='center', fontsize=10, color='black')\n",
    "\n",
    "# Create a horizontal bar graph on the second subplot\n",
    "colors = plt.cm.viridis(top_zones_10['counts'] / top_zones_10['counts'].max())\n",
    "ax2.barh(top_zones_10['Zone'], top_zones_10['counts'], color=colors)\n",
    "ax2.set_title('Top 10 Zones', fontdict={'fontsize': 16})\n",
    "ax2.set_xlabel('Total Pickup Counts')\n",
    "ax2.invert_yaxis()  # Reverse the order to have the highest at the top\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['left'].set_visible(False)\n",
    "ax2.spines['bottom'].set_visible(False)\n",
    "\n",
    "# Create an overall title for the figure\n",
    "fig.suptitle('Manhattan Taxi Pickup Analysis', fontsize=24, x = 0.64, y=1.05)\n",
    "\n",
    "# Make the bar graph align with the colormap of the map\n",
    "sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=gdf_zones['counts'].min(), vmax=gdf_zones['counts'].max()))\n",
    "sm._A = []  # Necessary for matplotlib < 3.1\n",
    "cbar = fig.colorbar(sm, orientation='horizontal', fraction=0.05, pad=0.05)\n",
    "cbar.set_label('Pickup Counts')\n",
    "\n",
    "# Display the figure\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee0f3bd-720d-42d8-9002-8b3c1d79ef16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Pick Up Time Category Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dc21663-3d8d-49d1-9a61-c4c07f7e00dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3.1 Time of the Day Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ef6c0b-db0e-4400-8e2a-03e651532711",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Extract the hour from the pickup time\n",
    "from pyspark.sql.functions import hour, col, when\n",
    "\n",
    "Man_df_new = Man_df.withColumn('hour', hour('tpepPickupDateTime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7271ca7c-694c-4a61-aab4-701cea3a9922",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Categorize the hours into time of day categories\n",
    "time_categories = [\n",
    "    (col('hour') < 5, 'Midnight'),\n",
    "    (col('hour').between(5, 11), 'Morning'),\n",
    "    (col('hour').between(11, 15), 'Noon'),\n",
    "    (col('hour').between(15, 20), 'Evening'),\n",
    "    (col('hour') >= 20, 'Night')\n",
    "]\n",
    "\n",
    "Man_df_new = Man_df_new.withColumn('time_of_day', when(time_categories[0][0], time_categories[0][1])\n",
    "                                    .when(time_categories[1][0], time_categories[1][1])\n",
    "                                    .when(time_categories[2][0], time_categories[2][1])\n",
    "                                    .when(time_categories[3][0], time_categories[3][1])\n",
    "                                    .when(time_categories[4][0], time_categories[4][1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9826a1c2-54bf-4af7-a848-59be2e3b4cb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Group by the time category and count the number of trips\n",
    "time_of_day_counts = Man_df_new.groupBy('time_of_day').count().orderBy('time_of_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20778cbf-de07-443d-a979-58cba582dd69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Calculate the proportions\n",
    "total_trips = Man_df_new.count()\n",
    "time_of_day_props = time_of_day_counts.withColumn('proportion', col('count') / total_trips)\n",
    "\n",
    "# Collect the data to use for plotting\n",
    "time_of_day_data = time_of_day_props.collect()\n",
    "\n",
    "# Extract the time categories and proportions\n",
    "categories = [row['time_of_day'] for row in time_of_day_data]\n",
    "proportions = [row['proportion'] for row in time_of_day_data]\n",
    "counts = [row['count'] for row in time_of_day_data]  # for displaying the count on the chart\n",
    "\n",
    "# Convert proportions to percentages\n",
    "percentages = [p * 100 for p in proportions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d1c10a7-4e63-4b3c-9418-f2ebf2b6ba67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update categories with time ranges\n",
    "time_ranges = {\n",
    "    'Midnight': 'Midnight (0-5)',\n",
    "    'Morning': 'Morning (5-11)',\n",
    "    'Noon': 'Noon (11-15)',\n",
    "    'Evening': 'Evening (15-20)',\n",
    "    'Night': 'Night (20-24)'\n",
    "}\n",
    "categories_with_time = [time_ranges[cat] for cat in categories]\n",
    "\n",
    "# Identify the index of the largest segment\n",
    "largest_segment_index = proportions.index(max(proportions))\n",
    "\n",
    "# Define the explode values for the segments\n",
    "explode_values = [0.07 if i == largest_segment_index else 0 for i in range(len(categories))]\n",
    "\n",
    "# Create the donut chart\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "wedges, texts, autotexts = ax.pie(percentages, \n",
    "                                  labels=categories_with_time, \n",
    "                                  autopct=lambda pct: f\"{pct:.2f}%\", \n",
    "                                  startangle=140, \n",
    "                                  colors=plt.cm.tab20.colors, \n",
    "                                  pctdistance=0.85,\n",
    "                                  explode=explode_values)\n",
    "\n",
    "# Draw a circle at the center to turn it into a donut chart\n",
    "centre_circle = plt.Circle((0, 0), 0.60, fc='white')\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "ax.axis('equal')  \n",
    "\n",
    "# Set the position and alignment of the percentage labels\n",
    "plt.setp(autotexts, size=12, weight=\"bold\", color=\"white\", va=\"center\")\n",
    "\n",
    "# Bold the largest segment text\n",
    "plt.setp(autotexts[largest_segment_index], size=12,  color='white')\n",
    "\n",
    "# Add the total in the center\n",
    "total_trips_str = f'{total_trips:,}'  # formatted with a thousands separator\n",
    "plt.text(0, 0, total_trips_str, ha='center', va='center', fontsize=14, weight='bold')\n",
    "\n",
    "plt.title('Distribution of Trips by Time',fontsize=18, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3c32266-cfd6-4ce6-85d6-da38d51985d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3.2 Day of the Week Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fc0f55c-c980-495a-b5cc-bf6390ac23e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# Step 1\n",
    "\n",
    "# Add a column that extracts the date from the datetime\n",
    "Man_df_new = Man_df_new.withColumn('date', F.to_date('tpepPickupDateTime'))\n",
    "\n",
    "# Add a new column for the day of the week\n",
    "Man_df_new = Man_df_new.withColumn('day_of_week', date_format('tpepPickupDateTime', 'E'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e60288e3-66c9-4408-8464-ea899d6bdabd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Aggregate data for total and average number of trips per day of the week\n",
    "\n",
    "# Count the unique dates for each day of the week\n",
    "day_counts = Man_df_new.groupBy('day_of_week').agg(F.countDistinct('date').alias('day_count'))\n",
    "\n",
    "day_of_week_df = Man_df_new.groupBy('day_of_week').count().withColumnRenamed('count', 'total_trips')\n",
    "\n",
    "# Join this back with the total trips to calculate the average\n",
    "day_of_week_df = day_of_week_df.join(day_counts, 'day_of_week')\n",
    "day_of_week_df = day_of_week_df.withColumn('avg_trips', F.col('total_trips') / F.col('day_count'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53c65a75-4cf0-4bc8-b267-3b29b311e93a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Collect the data to use for plotting\n",
    "day_of_week_data = day_of_week_df.collect()\n",
    "\n",
    "# Sort the data based on the day of the week assuming order is Sun, Mon, Tue, etc.\n",
    "sorted_days = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
    "sorted_data = sorted(day_of_week_data, key=lambda x: sorted_days.index(x['day_of_week']))\n",
    "\n",
    "# Extract the day names and trip counts\n",
    "day_names = [row['day_of_week'] for row in sorted_data]\n",
    "total_counts = [row['total_trips'] for row in sorted_data]\n",
    "avg_counts = [row['avg_trips'] for row in sorted_data]\n",
    "proportions = [row['total_trips'] / total_trips for row in sorted_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56093497-c7fb-45cc-a8bd-e72c64ebe859",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define colors for each day to be consistent across both plots\n",
    "day_colors = plt.cm.Paired(range(len(sorted_days)))\n",
    "\n",
    "# Step 3: Create the combined plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar chart for average trips\n",
    "bars = ax1.bar(day_names, avg_counts, color=day_colors)\n",
    "ax1.set_title('Weekly Average')\n",
    "ax1.set_xlabel('Day of week')\n",
    "ax1.set_ylabel('Average number of trips')\n",
    "\n",
    "# Add the text labels above the bars\n",
    "for bar, label in zip(bars, avg_counts):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width() / 2, height, f'{label:.0f}', ha='center', va='bottom')\n",
    "\n",
    "# Remove the top and right spines\n",
    "ax = plt.gca()\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['left'].set_visible(False)\n",
    "ax1.spines['bottom'].set_visible(False)\n",
    "\n",
    "###########################################################################\n",
    "# Identify the index of the largest segment\n",
    "largest_segment_index = proportions.index(max(proportions))\n",
    "\n",
    "# Define the explode values for the segments\n",
    "explode_values = [0.07 if i == largest_segment_index else 0 for i in range(len(day_names))]\n",
    "\n",
    "# Donut chart for proportion of total trips\n",
    "wedges, texts, autotexts = ax2.pie(proportions, \n",
    "                                   labels=day_names, \n",
    "                                   autopct='%1.1f%%', \n",
    "                                   startangle=140, \n",
    "                                   pctdistance=0.85, \n",
    "                                   colors=day_colors,\n",
    "                                   explode=explode_values)\n",
    "\n",
    "centre_circle = plt.Circle((0, 0), 0.50, fc='white')\n",
    "fig.gca().add_artist(centre_circle)\n",
    "ax2.set_title('Proportion of Total Trips')\n",
    "\n",
    "# Adjust the size of percentage labels\n",
    "plt.setp(autotexts, size=12, weight=\"bold\", color = 'white')\n",
    "\n",
    "# Add the total in the center of the donut chart\n",
    "total_trips_str = f'{total_trips:,}'\n",
    "plt.text(0, 0, f'Total\\n{total_trips_str}', ha='center', va='center', fontsize=12, weight='bold')\n",
    "\n",
    "# Overall title for all subplots\n",
    "plt.suptitle('Weekly Demand Analysis', fontsize=18, fontweight='bold', y=1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e8ddebd-d753-4311-98ec-1db4d7078790",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Passenger Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "954ba09f-f93d-45f1-a846-2bc3b204e6e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate passenger count\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Aggregate passenger count\n",
    "passenger_count_df = Man_df_new.groupBy(\"passengerCount\").agg(count(\"*\").alias(\"tripCount\"))\n",
    "\n",
    "# Collect the data to the driver node (as a list of rows)\n",
    "passenger_count_data = passenger_count_df.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "586bd491-9971-42d3-9b35-0bcaf716b15c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(passenger_count_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd33b92c-9353-465d-8999-f0cbe54fc0ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract the passenger counts and corresponding trip counts\n",
    "passenger_counts = [row['passengerCount'] for row in passenger_count_data]\n",
    "trip_counts = [row['tripCount'] for row in passenger_count_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e35eeb64-9a5c-4d4b-98ac-f9e8f98b8e4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming passenger_count_data is a list of dictionaries with 'passengerCount' and 'tripCount' keys\n",
    "passenger_counts = [int(row['passengerCount']) for row in passenger_count_data]\n",
    "trip_counts = [row['tripCount'] for row in passenger_count_data]\n",
    "\n",
    "# Combine and sort data based on passenger counts as integers\n",
    "combined_data = sorted(zip(passenger_counts, trip_counts), key=lambda x: x[0])\n",
    "\n",
    "# Split combined_data back into passenger_counts and trip_counts, converting passenger counts back to strings\n",
    "sorted_passenger_counts, sorted_trip_counts = zip(*[(str(pc), tc) for pc, tc in combined_data])\n",
    "\n",
    "# Plot the bar graph with ordered x-axis\n",
    "colors = ['red' if pc == '0' or int(pc) > 6 else 'lightgreen' for pc in sorted_passenger_counts]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sorted_passenger_counts, sorted_trip_counts, color=colors, edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "\n",
    "plt.xlabel('Passenger Count')\n",
    "plt.ylabel('Trip Count')\n",
    "plt.title('Passenger Count Per Taxi', fontsize=16, fontweight='bold')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Remove spines but leave the bottom spine for grounding\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_color('#DDDDDD')  # Light color for bottom spine\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b201dfb2-c691-4857-92d7-ad0c9cce42de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Analysis Result: Remove outliers - 0, 7, 8, 9, 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d34b395a-f1e1-41db-9f6d-7956ada12503",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter out the unwanted passenger counts\n",
    "Man_df_new = Man_df_new.filter((col(\"passengerCount\") != 0) & (col(\"passengerCount\") != 7) & (col(\"passengerCount\") != 8) & (col(\"passengerCount\") != 9) & (col(\"passengerCount\") != 192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee44d20-1ec1-4665-b575-2ec8f99117a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Man_df_new.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a66de631-d0c1-43bb-9d04-44fb37b589d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5. Trip Distance and Duration Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a9203bb-55ba-4df0-918a-2c2fbb52249f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Goal 1: Keep only trip duration greater than 1 minute and less than 720 minute (12 hours)\n",
    "- Goal 2: Kepp only trip distance greater than 0 miles and less than the 99.9% percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05543c76-4170-49ad-937a-985b35007ee7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp,round\n",
    "\n",
    "# Calculate the duration in minutes\n",
    "Man_df_new = Man_df_new.withColumn(\n",
    "    'tripDuration (min)',\n",
    "    round(\n",
    "        (unix_timestamp(col('tpepDropoffDateTime')) - unix_timestamp(col('tpepPickupDateTime')))/60,\n",
    "    2))\n",
    "\n",
    "Man_df_new = Man_df_new.withColumn(\n",
    "    'tripSpeed (mph)',\n",
    "    round(col('tripDistance') / (col('tripDuration (min)') / 60)).cast('integer'))\n",
    "\n",
    "# Select only the tripDistance and tripDuration\n",
    "# TripFocused = TripFocused.select(TripFocused[\"tripDistance\"], col(\"tripDuration\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c376fe8-7d86-46f1-b111-83a3670ad42f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, min, max, stddev, col\n",
    "\n",
    "# Calculate basic statistics for tripDistance and tripDuration\n",
    "stats_distance = Man_df_new.select(\n",
    "    mean(col(\"tripDistance\")).alias(\"mean_distance\"),\n",
    "    stddev(col(\"tripDistance\")).alias(\"stddev_distance\"),\n",
    "    min(col(\"tripDistance\")).alias(\"min_distance\"),\n",
    "    max(col(\"tripDistance\")).alias(\"max_distance\"),\n",
    "    )\n",
    "\n",
    "stats_duration = Man_df_new.select(\n",
    "    mean(col(\"tripDuration (min)\")).alias(\"mean_duration\"),\n",
    "    stddev(col(\"tripDuration (min)\")).alias(\"stddev_duration\"),\n",
    "    min(col(\"tripDuration (min)\")).alias(\"min_duration\"),\n",
    "    max(col(\"tripDuration (min)\")).alias(\"max_duration\"),\n",
    "    )\n",
    "\n",
    "stats_speed = Man_df_new.select(\n",
    "    mean(col(\"tripSpeed (mph)\")).alias(\"mean_speed\"),\n",
    "    stddev(col(\"tripSpeed (mph)\")).alias(\"stddev_speed\"),\n",
    "    min(col(\"tripSpeed (mph)\")).alias(\"min_speed\"),\n",
    "    max(col(\"tripSpeed (mph)\")).alias(\"max_speed\"),\n",
    "    )\n",
    "\n",
    "# Show the stats\n",
    "stats_distance.show()\n",
    "stats_duration.show()\n",
    "stats_speed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2b5957b-0d98-49f6-a731-0c1389b57e68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5.1 Duration Outlier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5579a44-a69e-4319-a10a-7247479606a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import percent_rank\n",
    "\n",
    "# Calculate percentile ranks\n",
    "window = Window.orderBy(Man_df_new['tripDuration (min)'])\n",
    "df_with_percentile = Man_df_new.withColumn(\"percentile_rank\", percent_rank().over(window))\n",
    "\n",
    "# Now find the tripDuration at each percentile from 0 to 100\n",
    "percentiles = [i / 100 for i in range(101)]  # 0, 0.01, 0.02, ..., 1\n",
    "percentile_values = df_with_percentile.stat.approxQuantile(\"tripDuration (min)\", percentiles, 0.01)\n",
    "\n",
    "# Print the percentiles and their corresponding tripDuration\n",
    "for percentile, value in zip(percentiles, percentile_values):\n",
    "    print(f\"{percentile * 100} percentile value is {value}\")\n",
    "\n",
    "# To look more closely at the 90th to 100th percentile\n",
    "detailed_percentiles = [i / 100 for i in range(90, 101)]\n",
    "detailed_percentile_values = df_with_percentile.stat.approxQuantile(\"tripDuration (min)\", detailed_percentiles, 0.01)\n",
    "\n",
    "for percentile, value in zip(detailed_percentiles, detailed_percentile_values):\n",
    "    print(f\"{percentile * 100} percentile value is {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "723a5808-357a-4ed1-b110-8f5c73ba07b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "# Calculate percentile ranks\n",
    "window = Window.orderBy(Man_df_new['tripDuration (min)'])\n",
    "df_with_percentile = Man_df_new.withColumn(\"percentile_rank\", percent_rank().over(window))\n",
    "\n",
    "# Calculate detailed percentiles for tripDuration\n",
    "detailed_percentiles = [float(i) / 1000 + 0.98 for i in range(1, 11)]\n",
    "detailed_percentile_values = df_with_percentile.approxQuantile(\"tripDuration (min)\", detailed_percentiles, 0.01)\n",
    "\n",
    "# Print detailed percentiles and their corresponding tripDistance\n",
    "for percentile, value in zip(detailed_percentiles, detailed_percentile_values):\n",
    "    print(f\"{percentile * 100} percentile value is {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fc874b5-8b71-4215-832d-390c434b79db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5.2 Distance Outlier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9b9c18-4029-4c5b-9ca8-5db3df1d7211",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import percent_rank\n",
    "\n",
    "# Calculate percentile ranks\n",
    "window = Window.orderBy(Man_df_new['tripDistance'])\n",
    "df_with_percentile = Man_df_new.withColumn(\"percentile_rank\", percent_rank().over(window))\n",
    "\n",
    "# Now find the tripDistance at each percentile from 0 to 100\n",
    "percentiles = [i / 100 for i in range(101)]  # 0, 0.01, 0.02, ..., 1\n",
    "percentile_values = df_with_percentile.stat.approxQuantile(\"tripDistance\", percentiles, 0.01)\n",
    "\n",
    "# Print the percentiles and their corresponding tripDuration\n",
    "for percentile, value in zip(percentiles, percentile_values):\n",
    "    print(f\"{percentile * 100} percentile value is {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb82137-662b-4cf0-bfa1-89b4f3394c4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Calculate detailed percentiles for tripDistance\n",
    "detailed_percentiles = [float(i) / 1000 + 0.98 for i in range(1, 11)]\n",
    "detailed_percentile_values = df_with_percentile.approxQuantile(\"tripDistance\", detailed_percentiles, 0.01)\n",
    "\n",
    "# Print detailed percentiles and their corresponding tripDistance\n",
    "for percentile, value in zip(detailed_percentiles, detailed_percentile_values):\n",
    "    print(f\"{percentile * 100} percentile value is {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68eb642f-7785-4c23-ac1a-f92be88b7283",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5.3 Speed Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df20d735-a96e-4d3b-82e1-af232c941920",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import percent_rank\n",
    "\n",
    "# Calculate percentile ranks\n",
    "window = Window.orderBy(Man_df_new['tripSpeed (mph)'])\n",
    "df_with_percentile = Man_df_new.withColumn(\"percentile_rank\", percent_rank().over(window))\n",
    "\n",
    "# Now find the tripDistance at each percentile from 0 to 100\n",
    "percentiles = [i / 100 for i in range(101)]  # 0, 0.01, 0.02, ..., 1\n",
    "percentile_values = df_with_percentile.stat.approxQuantile(\"tripSpeed (mph)\", percentiles, 0.01)\n",
    "\n",
    "# Print the percentiles and their corresponding tripDuration\n",
    "for percentile, value in zip(percentiles, percentile_values):\n",
    "    print(f\"{percentile * 100} percentile value is {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71de69b0-25c3-464c-a7b9-efce85b46a5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Calculate detailed percentiles for tripDistance\n",
    "detailed_percentiles = [float(i) / 1000 + 0.98 for i in range(1, 11)]\n",
    "detailed_percentile_values = df_with_percentile.approxQuantile(\"tripSpeed (mph)\", detailed_percentiles, 0.01)\n",
    "\n",
    "# Print detailed percentiles and their corresponding tripDistance\n",
    "for percentile, value in zip(detailed_percentiles, detailed_percentile_values):\n",
    "    print(f\"{percentile * 100} percentile value is {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d845c757-c9b9-41cb-877d-8fb9fd0f5e91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5.4 Trip Outlier Count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0054da25-34d0-434e-b3a0-b8726aad5bd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming Man_df_new is your DataFrame already loaded in Spark\n",
    "Man_df_new.createOrReplaceTempView(\"Man_df_new\")\n",
    "\n",
    "# Retrieve the data needed for plotting\n",
    "query = \"\"\"\n",
    "SELECT `tripDistance`, `tripDuration (min)`, `tripSpeed (mph)`\n",
    "FROM Man_df_new\n",
    "\"\"\"\n",
    "plot_data = spark.sql(query).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d70ca1-01f1-4f26-9cac-afa36f6ba8fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create subplots for boxplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Boxplot for tripDistance\n",
    "axs[0].boxplot(plot_data['tripDistance'])\n",
    "axs[0].set_title('Trip Distance (miles)')\n",
    "\n",
    "# Boxplot for Trip Duration - filter out negative and zero durations before plotting\n",
    "#filtered_durations = plot_data['tripDuration (min)'][plot_data['tripDuration (min)'] > 0]\n",
    "axs[1].boxplot(plot_data['tripDuration (min)'])\n",
    "axs[1].set_title('Trip Duration (min)')\n",
    "\n",
    "\n",
    "# Boxplot for Trip Speed - remove the log scale due to negative values\n",
    "filtered_speeds = plot_data['tripSpeed (mph)'][plot_data['tripSpeed (mph)'] > 0]\n",
    "axs[2].boxplot(filtered_speeds)\n",
    "axs[2].set_title('Trip Speed (mph)')\n",
    "\n",
    "# Set common labels\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Dataset')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d150652-6a9c-4c69-a0e9-4843801de124",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Enable Arrow-based columnar data transfers; this makes toPandas() faster for large datasets\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# Sample the data\n",
    "sampled_data = Man_df_new.sample(withReplacement=False, fraction=0.01)  # for example, 1% of the data\n",
    "\n",
    "# Sample the data and convert to Pandas DataFrame\n",
    "sampled_pd = sampled_data.toPandas()\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = sampled_pd['tripDistance']\n",
    "y = sampled_pd['tripDuration (min)']\n",
    "z = sampled_pd['tripSpeed (mph)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b6fd873-29d9-43bd-b8d0-1e82a774bd74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Create a 3D scatter plot with improved aesthetics\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Color by speed, normalize the color\n",
    "colors = sampled_pd['tripSpeed (mph)']\n",
    "norm = plt.Normalize(colors.min(), colors.max())\n",
    "\n",
    "# Create a scatter plot\n",
    "sc = ax.scatter(x, y, z, c=colors, cmap='viridis', norm=norm, edgecolors='w', s=50)\n",
    "\n",
    "# Add color bar which maps values to colors\n",
    "cbar = fig.colorbar(sc, shrink=0.5, aspect=20, pad=0.1)\n",
    "cbar.set_label('Trip Speed (mph)')\n",
    "\n",
    "# Set labels with improved font size\n",
    "ax.set_xlabel('Trip Distance (miles)', fontsize=12, labelpad=10)\n",
    "ax.set_ylabel('Trip Duration (min)', fontsize=12, labelpad=10)\n",
    "ax.set_zlabel('Trip Speed (mph)', fontsize=12, labelpad=10)\n",
    "\n",
    "# Set background to white\n",
    "ax.w_xaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))\n",
    "ax.w_yaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))\n",
    "ax.w_zaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))\n",
    "\n",
    "# Set title with improved aesthetics\n",
    "ax.set_title('Sampled Trip Characteristics', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37fd2fa3-65fd-4fc9-80c1-76c9a7e0a432",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Create a 3D scatter plot with improved aesthetics\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Color by speed, normalize the color\n",
    "colors = sampled_pd['tripSpeed (mph)']\n",
    "norm = plt.Normalize(colors.min(), colors.max())\n",
    "\n",
    "# Create a scatter plot\n",
    "sc = ax.scatter(x, y, z, c=colors, cmap='viridis', norm=norm, edgecolors='w', s=50)\n",
    "\n",
    "# Add color bar which maps values to colors\n",
    "cbar = fig.colorbar(sc, shrink=0.5, aspect=20, pad=0.1)\n",
    "cbar.set_label('Trip Speed (mph)')\n",
    "\n",
    "# Set labels with improved font size\n",
    "ax.set_xlabel('Trip Distance (miles)', fontsize=12, labelpad=10)\n",
    "ax.set_ylabel('Trip Duration (min)', fontsize=12, labelpad=10)\n",
    "ax.set_zlabel('Trip Speed (mph)', fontsize=12, labelpad=10)\n",
    "\n",
    "# Set title with improved aesthetics\n",
    "ax.set_title('Sampled Trip Characteristics', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74f9d6b5-342f-47d8-8de1-5f916f5d7ffe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Assuming outlier is any tripDuration over 12 hours (720 minutes) and less than 1 minute \n",
    "outliers_duration = Man_df_new.filter((col(\"tripDuration (min)\") <1 ) | (col(\"tripDuration (min)\") > 720))\n",
    "outliers_count = outliers_duration.count()\n",
    "print(f\"Number of duration outliers (trips over 12 hours or less than 1 min): {outliers_count}\")\n",
    "\n",
    "outliers_duration_2 = Man_df_new.filter((col(\"tripDuration (min)\") <1 ) | (col(\"tripDuration (min)\") > 45))\n",
    "outliers_count = outliers_duration_2.count()\n",
    "print(f\"Number of duration outliers (less than 1 min or over 45 min): {outliers_count}\")\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "# Trip Distance less than 0 miles and greater than 12 miles\n",
    "outliers_distance = Man_df_new.filter((col(\"tripDistance\") < 0) | (col(\"tripDistance\") > 12))\n",
    "outliers_count = outliers_distance.count()\n",
    "print(f\"Number of distance outliers (less than 0 miles or over 12 miles): {outliers_count}\")\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "# Trip speed less than 1 mph and greater than 30 mph\n",
    "outliers_speed = Man_df_new.filter((col(\"tripSpeed (mph)\") < 1) | (col(\"tripSpeed (mph)\") > 30))\n",
    "outliers_count = outliers_speed.count()\n",
    "print(f\"Number of speed outliers (less than 1 mph and greater than 30 mph): {outliers_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cac811a0-3920-4785-8d32-eb4823d709d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6. Total Fare Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d9ec120-b401-4cbc-bc24-e067c0a92281",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, min, max, stddev, col\n",
    "stats_Amount = Man_df_new.select(\n",
    "    mean(col(\"totalAmount\")).alias(\"mean_Amount\"),\n",
    "    stddev(col(\"totalAmount\")).alias(\"stddev_Amount\"),\n",
    "    min(col(\"totalAmount\")).alias(\"min_Amount\"),\n",
    "    max(col(\"totalAmount\")).alias(\"max_Amount\"),\n",
    "    )\n",
    "\n",
    "stats_Amount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9f94e00-4f72-470a-85cc-770241d50cde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import percent_rank\n",
    "\n",
    "# Calculate percentile ranks\n",
    "window = Window.orderBy(Man_df_new['totalAmount'])\n",
    "df_with_percentile = Man_df_new.withColumn(\"percentile_rank\", percent_rank().over(window))\n",
    "\n",
    "# Now find the tripDistance at each percentile from 0 to 100\n",
    "percentiles = [i / 100 for i in range(101)]  # 0, 0.01, 0.02, ..., 1\n",
    "percentile_values = df_with_percentile.stat.approxQuantile(\"totalAmount\", percentiles, 0.01)\n",
    "\n",
    "# Print the percentiles and their corresponding tripDuration\n",
    "for percentile, value in zip(percentiles, percentile_values):\n",
    "    print(f\"{percentile * 100} percentile value is {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f58579a8-81ca-430f-ab04-f893f950cef9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Calculate detailed percentiles for tripDistance\n",
    "detailed_percentiles = [float(i) / 1000 + 0.98 for i in range(1, 11)]\n",
    "detailed_percentile_values = df_with_percentile.approxQuantile(\"totalAmount\", detailed_percentiles, 0.01)\n",
    "\n",
    "# Print detailed percentiles and their corresponding tripDistance\n",
    "for percentile, value in zip(detailed_percentiles, detailed_percentile_values):\n",
    "    print(f\"{percentile * 100} percentile value is {value}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3790654468691509,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Taxi_2017_EDA_Cleaning",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
