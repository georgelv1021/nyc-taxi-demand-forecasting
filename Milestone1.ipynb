{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries needed\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = SparkSession.builder.appName(\"FinalProject\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remote blob path: wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow\n",
      "Register the DataFrame as a SQL temporary view: source\n",
      "Displaying top 10 rows: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[vendorID: string, tpepPickupDateTime: timestamp, tpepDropoffDateTime: timestamp, passengerCount: int, tripDistance: double, puLocationId: string, doLocationId: string, startLon: double, startLat: double, endLon: double, endLat: double, rateCodeId: int, storeAndFwdFlag: string, paymentType: string, fareAmount: double, extra: double, mtaTax: double, improvementSurcharge: string, tipAmount: double, tollsAmount: double, totalAmount: double, puYear: int, puMonth: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data : https://learn.microsoft.com/en-us/azure/open-datasets/dataset-taxi-yellow?tabs=pyspark#azure-databricks\n",
    "\n",
    "# Azure storage access info\n",
    "blob_account_name = \"azureopendatastorage\"\n",
    "blob_container_name = \"nyctlc\"\n",
    "blob_relative_path = \"yellow\"\n",
    "blob_sas_token = \"r\"\n",
    "\n",
    "# Allow SPARK to read from Blob remotely\n",
    "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
    "spark.conf.set(\n",
    "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
    "  blob_sas_token)\n",
    "print('Remote blob path: ' + wasbs_path)\n",
    "\n",
    "# SPARK read parquet, note that it won't load any data yet by now\n",
    "df = spark.read.parquet(wasbs_path)\n",
    "print('Register the DataFrame as a SQL temporary view: source')\n",
    "df.createOrReplaceTempView('source')\n",
    "\n",
    "# Display top 10 rows\n",
    "print('Displaying top 10 rows: ')\n",
    "display(spark.sql('SELECT * FROM source LIMIT 10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+--------------+------------+------------+------------+----------+---------+----------+---------+----------+---------------+-----------+----------+-----+------+--------------------+---------+-----------+-----------+------+-------+\n",
      "|vendorID| tpepPickupDateTime|tpepDropoffDateTime|passengerCount|tripDistance|puLocationId|doLocationId|  startLon| startLat|    endLon|   endLat|rateCodeId|storeAndFwdFlag|paymentType|fareAmount|extra|mtaTax|improvementSurcharge|tipAmount|tollsAmount|totalAmount|puYear|puMonth|\n",
      "+--------+-------------------+-------------------+--------------+------------+------------+------------+----------+---------+----------+---------+----------+---------------+-----------+----------+-----+------+--------------------+---------+-----------+-----------+------+-------+\n",
      "|     CMT|2012-02-29 23:53:14|2012-03-01 00:00:43|             1|         2.1|        null|        null|-73.980494|40.730601|-73.983532|40.752311|         1|              N|        CSH|       7.3|  0.5|   0.5|                null|      0.0|        0.0|        8.3|  2012|      3|\n",
      "|     VTS|2012-03-17 08:01:00|2012-03-17 08:15:00|             1|       11.06|        null|        null|-73.986067|40.699862|-73.814838|40.737052|         1|           null|        CRD|      24.5|  0.0|   0.5|                null|      4.9|        0.0|       29.9|  2012|      3|\n",
      "|     CMT|2012-02-29 23:58:51|2012-03-01 00:15:48|             1|         3.4|        null|        null|-73.968967|40.754359|-73.957048|40.743289|         1|              N|        CRD|      12.5|  0.5|   0.5|                null|      1.5|        0.0|       15.0|  2012|      3|\n",
      "|     CMT|2012-03-01 19:24:16|2012-03-01 19:31:22|             1|         1.3|        null|        null| -73.99374| 40.75307|-74.005428|40.741118|         1|              N|        CRD|       6.1|  1.0|   0.5|                null|      0.0|        0.0|        7.6|  2012|      3|\n",
      "|     CMT|2012-02-29 23:46:32|2012-03-01 00:05:18|             3|         2.0|        null|        null|-73.973723|40.752323|-73.948275|40.769413|         1|              N|        CSH|      11.7|  0.5|   0.5|                null|      0.0|        0.0|       12.7|  2012|      3|\n",
      "|     VTS|2012-03-07 15:17:00|2012-03-07 15:26:00|             5|        1.87|        null|        null|-73.988237| 40.75929| -73.97114| 40.78275|         1|           null|        CSH|       7.7|  0.0|   0.5|                null|      0.0|        0.0|        8.2|  2012|      3|\n",
      "|     CMT|2012-02-29 23:41:58|2012-03-01 00:02:29|             1|        12.4|        null|        null|-73.954536|40.727742|-73.768994|40.760246|         1|              N|        CSH|      28.5|  0.5|   0.5|                null|      0.0|        0.0|       29.5|  2012|      3|\n",
      "|     VTS|2012-03-18 15:21:00|2012-03-18 15:32:00|             6|        2.51|        null|        null|-74.001705|40.732345|-73.974888|40.750835|         1|           null|        CSH|       8.9|  0.0|   0.5|                null|      0.0|        0.0|        9.4|  2012|      3|\n",
      "|     CMT|2012-02-29 23:47:08|2012-03-01 00:06:42|             4|         6.3|        null|        null|-73.992319|40.724503|-73.923589| 40.76113|         1|              N|        CRD|      16.5|  0.5|   0.5|                null|     4.37|        0.0|      21.87|  2012|      3|\n",
      "|     VTS|2012-03-13 22:26:00|2012-03-13 22:37:00|             1|        1.34|        null|        null|-74.009907|40.706292|-74.000512| 40.71733|         1|           null|        CSH|       7.3|  0.5|   0.5|                null|      0.0|        0.0|        8.3|  2012|      3|\n",
      "+--------+-------------------+-------------------+--------------+------------+------------+------------+----------+---------+----------+---------+----------+---------------+-----------+----------+-----+------+--------------------+---------+-----------+-----------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10) # get first 10 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller dataset bc data is big. Will use the truncated df for the project\n",
    "df_truncated = df.filter((col(\"puYear\") >= 2017) & (col(\"puYear\") <= 2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+--------------+------------+------------+------------+--------+--------+------+------+----------+---------------+-----------+----------+-----+------+--------------------+---------+-----------+-----------+------+-------+\n",
      "|vendorID| tpepPickupDateTime|tpepDropoffDateTime|passengerCount|tripDistance|puLocationId|doLocationId|startLon|startLat|endLon|endLat|rateCodeId|storeAndFwdFlag|paymentType|fareAmount|extra|mtaTax|improvementSurcharge|tipAmount|tollsAmount|totalAmount|puYear|puMonth|\n",
      "+--------+-------------------+-------------------+--------------+------------+------------+------------+--------+--------+------+------+----------+---------------+-----------+----------+-----+------+--------------------+---------+-----------+-----------+------+-------+\n",
      "|       1|2017-03-02 07:59:01|2017-03-02 08:12:48|             1|         1.4|         238|         236|    null|    null|  null|  null|         1|              N|          1|       9.5|  0.0|   0.5|                 0.3|      0.0|        0.0|       10.3|  2017|      3|\n",
      "|       2|2017-02-28 12:03:31|2017-03-01 11:58:41|             6|        0.87|         263|         140|    null|    null|  null|  null|         1|              N|          1|       5.5|  0.0|   0.5|                 0.3|      0.0|        0.0|        6.3|  2017|      3|\n",
      "|       1|2017-03-02 00:46:47|2017-03-02 00:52:13|             1|         0.9|          48|         100|    null|    null|  null|  null|         1|              N|          1|       5.5|  0.5|   0.5|                 0.3|     1.35|        0.0|       8.15|  2017|      3|\n",
      "|       2|2017-02-28 17:21:44|2017-03-01 16:26:31|             2|        11.4|         138|         170|    null|    null|  null|  null|         1|              N|          2|      34.5|  1.0|   0.5|                 0.3|      0.0|       5.54|      41.84|  2017|      3|\n",
      "|       2|2017-03-02 00:07:44|2017-03-02 00:26:32|             1|        9.27|         144|         179|    null|    null|  null|  null|         1|              N|          1|      27.5|  0.5|   0.5|                 0.3|     3.08|        0.0|      31.88|  2017|      3|\n",
      "|       2|2017-02-28 18:07:28|2017-03-01 17:54:58|             1|        0.02|         237|         237|    null|    null|  null|  null|         1|              N|          2|       3.0|  1.0|   0.5|                 0.3|      0.0|        0.0|        4.8|  2017|      3|\n",
      "|       2|2017-03-02 01:20:36|2017-03-02 01:46:38|             1|        7.38|         249|         129|    null|    null|  null|  null|         1|              N|          1|      24.0|  0.5|   0.5|                 0.3|     5.06|        0.0|      30.36|  2017|      3|\n",
      "|       2|2017-02-28 11:07:30|2017-03-01 10:58:22|             1|        1.11|         236|         140|    null|    null|  null|  null|         1|              N|          1|      10.0|  0.0|   0.5|                 0.3|     2.16|        0.0|      12.96|  2017|      3|\n",
      "|       2|2017-03-01 23:16:36|2017-03-01 23:27:08|             1|        2.51|         230|         263|    null|    null|  null|  null|         1|              N|          1|      10.0|  0.5|   0.5|                 0.3|     2.26|        0.0|      13.56|  2017|      3|\n",
      "|       2|2017-02-28 15:16:50|2017-03-01 15:15:28|             1|        1.46|         246|         161|    null|    null|  null|  null|         1|              N|          1|       8.5|  0.0|   0.5|                 0.3|     1.86|        0.0|      11.16|  2017|      3|\n",
      "+--------+-------------------+-------------------+--------------+------------+------------+------------+--------+--------+------+------+----------+---------------+-----------+----------+-----+------+--------------------+---------+-----------+-----------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the smaller subset\n",
    "df_truncated.show(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in filtered DF: 216300320\n"
     ]
    }
   ],
   "source": [
    "# count the number of rows in the truncated df\n",
    "row_count = df_truncated.count()\n",
    "print(\"Number of rows in filtered DF:\", row_count)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendorID: string (nullable = true)\n",
      " |-- tpepPickupDateTime: timestamp (nullable = true)\n",
      " |-- tpepDropoffDateTime: timestamp (nullable = true)\n",
      " |-- passengerCount: integer (nullable = true)\n",
      " |-- tripDistance: double (nullable = true)\n",
      " |-- puLocationId: string (nullable = true)\n",
      " |-- doLocationId: string (nullable = true)\n",
      " |-- startLon: double (nullable = true)\n",
      " |-- startLat: double (nullable = true)\n",
      " |-- endLon: double (nullable = true)\n",
      " |-- endLat: double (nullable = true)\n",
      " |-- rateCodeId: integer (nullable = true)\n",
      " |-- storeAndFwdFlag: string (nullable = true)\n",
      " |-- paymentType: string (nullable = true)\n",
      " |-- fareAmount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mtaTax: double (nullable = true)\n",
      " |-- improvementSurcharge: string (nullable = true)\n",
      " |-- tipAmount: double (nullable = true)\n",
      " |-- tollsAmount: double (nullable = true)\n",
      " |-- totalAmount: double (nullable = true)\n",
      " |-- puYear: integer (nullable = true)\n",
      " |-- puMonth: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema to inspect data types\n",
    "df_truncated.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the df to a CSV file in GCS\n",
    "bucket_name = 'final-bucketv6' # change based on bucket name created\n",
    "df_truncated.write.csv(f\"gs://{bucket_name}/notebooks/jupyter/Data/\")\n",
    "\n",
    "# This will create a Data folder in GSC and will partion the data into 54 different CSV files due to size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Explain data (i.e.., simple exploratory analysis of various fields, such as the semantic as well the intrinsic meaning of ranges, null values, categorical/numerical, mean/std.dev to normalize and/or scale inputs). Identify any missing or corrupt (i.e., outlier) data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Dictonary. Can also be found at https://learn.microsoft.com/en-us/azure/open-datasets/dataset-taxi-yellow?tabs=pyspark#azure-databricks\n",
    "\n",
    "The dictonary is organized by the following:\n",
    "variable name, data type, number of unique values, description (optional)\n",
    "- doLocationId, string, 265 unique values, TLC Taxi Zone in which the taximeter was disengaged.\n",
    "- endLat, double, 961,994 unique values\n",
    "- endLon, double, 1,144,935 unique values\n",
    "- extra, double, 877 unique values, Miscellaneous extras and surcharges. Currently, this only includes the 0.50 and 1 rush hour and overnight charges. (so only values it contains are 0.5 and 1.0)\n",
    "- fareAmount, double, 18,935 unique values, The time-and-distance fare calculated by the meter.\n",
    "- improvementSurcharge, string, 60 unique values, 0.30 improvement surcharge assessed trips at the flag drop. The improvement surcharge began being levied in 2015.\n",
    "- mtaTax, double, 360 unique values, $0.50 MTA tax that is automatically triggered based on the metered rate in use.\n",
    "- passengerCount, int, 64 unique values, The number of passengers in the vehicle. This is a driver-entered value.\n",
    "- paymentType, string, 6,282 unique values, A numeric code signifying how the passenger paid for the trip. 1= Credit card; 2= Cash; 3= No charge; 4= Dispute; 5= Unknown; 6= Voided trip.\n",
    "- puLocationId, string, 266 unique values, TLC Taxi Zone in which the taximeter was engaged.\n",
    "- puMonth, int, 12 unique values\n",
    "- puYear, int, 29 unique values\n",
    "- rateCodeId, int, 56 unique values, The final rate code in effect at the end of the trip. 1= Standard rate; 2= JFK; 3= Newark; 4= Nassau or Westchester; 5= Negotiated fare; 6= Group ride.\n",
    "- startLat, double, 833,016 unique values\n",
    "- startLon, double, 957,428 unique value\n",
    "- storeAndFwdFlag, string, 8 unique values, This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, also known as “store and forward,” because the vehicle did not have a connection to the server. Y= store and forward trip; N= not a store and forward trip.\n",
    "- tipAmount, double, 12,121 unique values, This field is automatically populated for credit card tips. Cash tips are not included.\n",
    "- tollsAmount, double, 6,634 unique values, Total amount of all tolls paid in trip.\n",
    "- totalAmount, double, 39,707 unique values, The total amount charged to passengers. Does not include cash tips.\n",
    "- tpepDropoffDateTime, timestamp, 290,185,010 unique values, The date and time when the meter was disengaged.\n",
    "- tpepPickupDateTime, timestamp, 289,948,585 unique values, The date and time when the meter was engaged.\n",
    "- tripDistance, double, 14,003 unique values, The elapsed trip distance in miles reported by the taximeter.\n",
    "- vendorID, string, 7 unique values, A code indicating the TPEP provider that provided the record. 1= Creative Mobile Technologies, LLC; 2= VeriFone Inc.\n",
    "- vendorID, int, 2 unique values, A code indicating the LPEP provider that provided the record. 1= Creative Mobile Technologies, LLC; 2= VeriFone Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-------------------+--------------+------------+------------+------------+---------+---------+---------+---------+----------+---------------+-----------+----------+-----+------+--------------------+---------+-----------+-----------+------+-------+\n",
      "|vendorID|tpepPickupDateTime|tpepDropoffDateTime|passengerCount|tripDistance|puLocationId|doLocationId| startLon| startLat|   endLon|   endLat|rateCodeId|storeAndFwdFlag|paymentType|fareAmount|extra|mtaTax|improvementSurcharge|tipAmount|tollsAmount|totalAmount|puYear|puMonth|\n",
      "+--------+------------------+-------------------+--------------+------------+------------+------------+---------+---------+---------+---------+----------+---------------+-----------+----------+-----+------+--------------------+---------+-----------+-----------+------+-------+\n",
      "|       0|                 0|                  0|             0|           0|           0|           0|216300320|216300320|216300320|216300320|         0|              0|          0|         0|    0|     0|                   0|        0|          0|          0|     0|      0|\n",
      "+--------+------------------+-------------------+--------------+------------+------------+------------+---------+---------+---------+---------+----------+---------------+-----------+----------+-----+------+--------------------+---------+-----------+-----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count missing values for each column\n",
    "missing_values_counts = df_truncated.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_truncated.columns])\n",
    "\n",
    "missing_values_counts.show()\n",
    "\n",
    "# we can see the following colums have missing values: startLon, startLat, endLon, and endLat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the following columns have missing values: startLon, startLat, endLon, and endLat. Since these variables (latitude and longitude) are not crucial to our project we will not need to handle this missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------+\n",
      "|puYear|puMonth|   count|\n",
      "+------+-------+--------+\n",
      "|  2017|      3|10294628|\n",
      "|  2017|      8| 8422197|\n",
      "|  2017|     10| 9768740|\n",
      "|  2018|     10| 8821138|\n",
      "|  2018|      1| 8760120|\n",
      "|  2018|      3| 9429450|\n",
      "|  2018|      8| 7849041|\n",
      "|  2017|      7| 8588486|\n",
      "|  2018|      5| 9224095|\n",
      "|  2017|     12| 9508274|\n",
      "|  2017|      9| 8945575|\n",
      "|  2017|      4|10046188|\n",
      "|  2018|      9| 8039933|\n",
      "|  2018|      7| 7849585|\n",
      "|  2018|     12| 8172811|\n",
      "|  2018|     11| 8145749|\n",
      "|  2018|      6| 8713709|\n",
      "|  2017|      2| 9168825|\n",
      "|  2017|     11| 9284777|\n",
      "|  2017|      5|10102124|\n",
      "+------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count of tax rides per month\n",
    "\n",
    "# group by year and month then count the number of rides for each time period\n",
    "rides_count_per_month = df_truncated.groupBy(year(\"tpepPickupDateTime\").alias(\"puYear\"), \n",
    "                                             month(\"tpepPickupDateTime\").alias(\"puMonth\")).count()\n",
    "\n",
    "rides_count_per_month.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------+\n",
      "|puYear|puMonth|   count|\n",
      "+------+-------+--------+\n",
      "|  2018|     12| 8172811|\n",
      "|  2018|     11| 8145749|\n",
      "|  2018|     10| 8821138|\n",
      "|  2018|      9| 8039933|\n",
      "|  2018|      8| 7849041|\n",
      "|  2018|      7| 7849585|\n",
      "|  2018|      6| 8713709|\n",
      "|  2018|      5| 9224095|\n",
      "|  2018|      4| 9305295|\n",
      "|  2018|      3| 9429450|\n",
      "|  2018|      2| 8492461|\n",
      "|  2018|      1| 8760120|\n",
      "|  2017|     12| 9508274|\n",
      "|  2017|     11| 9284777|\n",
      "|  2017|     10| 9768740|\n",
      "|  2017|      9| 8945575|\n",
      "|  2017|      8| 8422197|\n",
      "|  2017|      7| 8588486|\n",
      "|  2017|      6| 9656993|\n",
      "|  2017|      5|10102124|\n",
      "|  2017|      4|10046188|\n",
      "|  2017|      3|10294628|\n",
      "|  2017|      2| 9168825|\n",
      "|  2017|      1| 9710126|\n",
      "+------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# order by year and month in descending order\n",
    "rides_count_per_month = rides_count_per_month.orderBy(\"puYear\", \"puMonth\", ascending=False)\n",
    "\n",
    "rides_count_per_month.show(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a bar chart for ride counts per month,year to visually see the data. This takes a while to load. \n",
    "# Did not run\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# extracting data to plot\n",
    "years = rides_count_per_month.select(\"puYear\").rdd.flatMap(lambda x: x).collect()\n",
    "months = rides_count_per_month.select(\"puMonth\").rdd.flatMap(lambda x: x).collect()\n",
    "counts = rides_count_per_month.select(\"count\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(counts)), counts, color='skyblue')\n",
    "plt.xlabel('Month-Year', fontsize=14)\n",
    "plt.ylabel('Number of Rides', fontsize=14)\n",
    "plt.title('Number of Taxi Rides per Month-Year', fontsize=16)\n",
    "plt.xticks(range(len(counts)), [f'{m}-{y}' for m, y in zip(months, years)], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------------+------+\n",
      "|puYear|puMonth|puLocationId| count|\n",
      "+------+-------+------------+------+\n",
      "|  2017|      5|         237|420877|\n",
      "|  2018|      5|         237|407246|\n",
      "|  2017|     10|         237|405086|\n",
      "|  2017|     12|         237|401288|\n",
      "|  2017|     11|         237|398611|\n",
      "|  2018|     10|         237|393569|\n",
      "|  2017|      4|         237|390697|\n",
      "|  2018|      4|         237|390523|\n",
      "|  2017|      5|         236|381377|\n",
      "|  2017|      3|         237|380997|\n",
      "|  2017|      1|         237|380663|\n",
      "|  2017|      6|         237|379701|\n",
      "|  2017|      3|         161|377452|\n",
      "|  2017|     10|         161|376038|\n",
      "|  2018|      3|         237|371555|\n",
      "|  2018|      4|         161|369959|\n",
      "|  2017|      5|         161|369772|\n",
      "|  2017|      4|         161|366820|\n",
      "|  2018|      3|         161|366648|\n",
      "|  2017|     12|         236|366262|\n",
      "|  2017|      1|         236|363649|\n",
      "|  2018|     11|         237|363062|\n",
      "|  2018|      5|         236|361821|\n",
      "|  2017|     11|         161|361249|\n",
      "+------+-------+------------+------+\n",
      "only showing top 24 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count of tax rides per zone by month\n",
    "\n",
    "# group by year, month, and puLocationId and then count the number of rides for each group\n",
    "rides_count_per_zone_month = df_truncated.groupBy(year(\"tpepPickupDateTime\").alias(\"puYear\"), \n",
    "                                                  month(\"tpepPickupDateTime\").alias(\"puMonth\"), \n",
    "                                                  \"puLocationId\").count()\n",
    "\n",
    "# order by count in descending order to see highest count first\n",
    "rides_count_per_zone_month = rides_count_per_zone_month.orderBy(\"count\", ascending=False)\n",
    "\n",
    "rides_count_per_zone_month.show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define the outcome (i.e., the evaluation metric and the target) precisely, including mathematical formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metric: Since this is a demand forecasting problem, metrics like RMSE (Root Mean Square Error) or MAPE (Mean Absolute Percentage Error) could be used to measure the accuracy of predictions against actual demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Variable: The target variable is the count of taxi trips per time period (hourly/daily) for each NYC taxi zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudo-code example for calculating RMSE (Root Mean Square Error) and MAPE (Mean Absolute Percentage Error) for \n",
    "# a time series model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Root Mean Square Error (RMSE) function\n",
    "def calculate_rmse(actual_values, predicted_values):\n",
    "    squared_errors = (actual_values - predicted_values) ** 2\n",
    "    mean_squared_error = np.mean(squared_errors)\n",
    "    rmse = np.sqrt(mean_squared_error)\n",
    "    return rmse\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE) function\n",
    "def calculate_mape(actual_values, predicted_values):\n",
    "    absolute_percentage_errors = np.abs((actual_values - predicted_values) / actual_values)\n",
    "    mean_absolute_percentage_error = np.mean(absolute_percentage_errors)\n",
    "    return mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. How do you ingest the data files and represent them efficiently? (think about different file formats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to ingest and represent data files is using Parquet. Parquet is a columnar storage format that is efficient \n",
    "for large-scale data files tailored for analytical purposes. This efficiency stems from its compression techniques \n",
    "and nested data organization. Parquet also excels in the querying process. Using tools like Apache Spark,\n",
    "which integrates with Parquet, ensures smooth data reading and processing operations.\n",
    "\n",
    "After conducting additional analysis, our team has chosen to process the data using PySpark DataFrames (as shown in the code chunks above). We imported our data into a folder in Google Cloud Storage (GCS). Initially, we attempted to write the data to a Parquet file and then read it back in, given that we initially read from a Parquet file. However, this approach proved to be iterative and less efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Summarized relevant datasets. Describe what tables to join. Describe the workflow for achieving the joins (what are the keys, type of join). Steps to deal with potential missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our project, we operate with a single master dataset, eliminating the necessity for table joins. To address potential missing values within this dataset, we can employ one of two strategies. The first involves removing rows containing missing data. Alternatively, we utilize imputation techniques, which entail filling in missing values with estimated substitutes. Simple imputation methods include replacing missing values with the mean, median, or mode of the respective feature. More advanced techniques include regression imputation, k-nearest neighbors (KNN) imputation, and predictive modeling approaches like decision trees or random forests which offer viable alternatives for handling missing data. Since the missing data we have is not needed for our time series analysis we do not need to deal with missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Checkpoint the data after joining to avoid wasting time and resources!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not needed since we are not joining multiple datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Summarize how to split the data train/validation/test - making sure no leaks occur; for example: normalize your data using the training statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data for our time series analysis requires careful consideration to maintain temporal order. Randomly shuffling the data can lead to data leakage so we will split the data based on dates like so:\n",
    "\n",
    "Training: 1/2017 - 2/2018\n",
    "\n",
    "Validation: 3/2018 - 6/2018\n",
    "\n",
    "Testing: 7/2018 - 12/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Credit assignment plan: describe a plan on how the work will be distributed across the team members and how credit will be assigned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignment questions were evenly distributed among team members. Once individuals finished their assigned questions, we implemented a rotation system where each member reviewed and provided feedback on the answers of their peers, ensuring editing where necessary. In addition to this plan for future milestones we will also implement code reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. STRETCH Goal for this milestone: code up some or all of the above tasks in rank order of items 1, 2, 3, 4, and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 basic eda \n",
    "\n",
    "2 pseudo-code for evaluation metrics \n",
    "\n",
    "3 read in data into pyspark df in the begining of this notebook\n",
    "\n",
    "4 not related to our project since no joins were needed\n",
    "\n",
    "5 not related to our project since no joins were needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
